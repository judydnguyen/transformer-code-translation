{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"java\"\n",
    "target=\"cs\"\n",
    "lr=1e-4\n",
    "batch_size=64\n",
    "beam_size=10\n",
    "source_length=320\n",
    "target_length=256\n",
    "output_dir=f\"saved_models/{source}-{target}/\"\n",
    "train_file=f\"data/train.java-cs.txt.{source},data/train.java-cs.txt.{target}\"\n",
    "dev_file=f\"data/valid.java-cs.txt.{source},data/valid.java-cs.txt.{target}\"\n",
    "epochs=2\n",
    "pretrained_model=\"microsoft/graphcodebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dfg_function={\n",
    "    'python': DFG_python,\n",
    "    'java': DFG_java,\n",
    "    'ruby': DFG_ruby,\n",
    "    'go': DFG_go,\n",
    "    'php': DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#load parsers\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    # print(Language)\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 source,\n",
    "                 target,\n",
    "                 lang\n",
    "                 ):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lang=lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass    \n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"    \n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "        root_node = tree.root_node  \n",
    "        tokens_index=tree_to_token_index(root_node)     \n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]  \n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)  \n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{}) \n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataflow_and_cfg(code, parser, lang):\n",
    "    \"\"\"\n",
    "    Extract dataflow (DFG) and control flow (CFG) from the code.\n",
    "    \"\"\"\n",
    "    # Remove comments\n",
    "    try:\n",
    "        code = remove_comments_and_docstrings(code, lang)\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing comments: {e}\")\n",
    "        pass\n",
    "\n",
    "    if lang == \"php\":\n",
    "        code = \"<?php\" + code + \"?>\"\n",
    "\n",
    "    code_tokens, dfg, cfg = [], [], []\n",
    "\n",
    "    try:\n",
    "        # Parse the code to obtain the AST\n",
    "        tree = parser[0].parse(bytes(code, 'utf8'))\n",
    "        root_node = tree.root_node\n",
    "\n",
    "        # Extract tokens\n",
    "        tokens_index = tree_to_token_index(root_node)\n",
    "        code_lines = code.split('\\n')\n",
    "        code_tokens = [index_to_code_token(x, code_lines) for x in tokens_index]\n",
    "\n",
    "        # Map tokens to positions\n",
    "        index_to_code = {idx: (i, token) for idx, (i, token) in enumerate(zip(tokens_index, code_tokens))}\n",
    "\n",
    "        # Extract dataflow\n",
    "        try:\n",
    "            DFG, _ = parser[1](root_node, index_to_code, {})\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting DFG: {e}\")\n",
    "            DFG = []\n",
    "\n",
    "        DFG = sorted(DFG, key=lambda x: x[1])\n",
    "\n",
    "        # Filter DFG to include only relevant indices\n",
    "        index_set = set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1]) != 0:\n",
    "                index_set.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                index_set.add(x)\n",
    "\n",
    "        filtered_DFG = [d for d in DFG if d[1] in index_set]\n",
    "        dfg = filtered_DFG\n",
    "\n",
    "        # Extract control flow (CFG)\n",
    "        cfg = extract_control_flow_edges(root_node)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing tree or extracting flows: {e}\")\n",
    "\n",
    "    return code_tokens, dfg, cfg\n",
    "\n",
    "\n",
    "def extract_control_flow_edges(root_node):\n",
    "    \"\"\"\n",
    "    Extract control flow edges (e.g., branching, loops) from the AST.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node):\n",
    "        if node.type == \"if_statement\":\n",
    "            # Extract the condition\n",
    "            condition_node = next(\n",
    "                (child for child in node.children if child.type == \"parenthesized_expression\"), None\n",
    "            )\n",
    "            # Extract the \"then\" block\n",
    "            then_node = next(\n",
    "                (child for child in node.children if child.type == \"block\"), None\n",
    "            )\n",
    "            # Add edges for condition -> then\n",
    "            if condition_node and then_node:\n",
    "                edges.append((condition_node.start_point, then_node.start_point))\n",
    "            # Extract the \"else\" block, if present\n",
    "            else_node = next(\n",
    "                (child for child in node.children if child.type == \"else_body\"), None\n",
    "            )\n",
    "            if condition_node and else_node:\n",
    "                edges.append((condition_node.start_point, else_node.start_point))\n",
    "        elif node.type in (\"for_statement\", \"while_statement\"):\n",
    "            # Extract loop condition and body\n",
    "            condition_node = next(\n",
    "                (child for child in node.children if child.type == \"parenthesized_expression\"), None\n",
    "            )\n",
    "            body_node = next(\n",
    "                (child for child in node.children if child.type == \"block\"), None\n",
    "            )\n",
    "            if condition_node and body_node:\n",
    "                edges.append((condition_node.start_point, body_node.start_point))\n",
    "        elif node.type == \"return_statement\":\n",
    "            # Add a return statement as a flow edge\n",
    "            edges.append((node.start_point, node.end_point))\n",
    "\n",
    "        # Recurse for all children\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Tokens: ['public', 'class', 'Factorial', '{', 'public', 'static', 'long', 'factorial', '(', 'int', 'n', ')', '{', 'if', '(', 'n', '<=', '1', ')', '{', 'return', '1', ';', '}', 'return', 'n', '*', 'factorial', '(', 'n', '-', '1', ')', ';', '}', '}']\n",
      "Data Flow Graph (DFG): [('factorial', 7, 'comesFrom', [], []), ('n', 10, 'comesFrom', [], []), ('n', 15, 'comesFrom', ['n'], [10]), ('n', 25, 'comesFrom', ['n'], [10]), ('factorial', 27, 'comesFrom', ['factorial'], [7]), ('n', 29, 'comesFrom', ['n'], [10])]\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "public class Factorial {\n",
    "    public static long factorial(int n) {\n",
    "        if (n <= 1) {\n",
    "            return 1;\n",
    "        }\n",
    "        return n * factorial(n - 1);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# code_tokens, dfg, cfg = extract_dataflow_and_cfg(code, parser, \"java\")\n",
    "code_tokens, dfg = extract_dataflow(code, parsers[\"java\"], \"java\")\n",
    "print(\"Code Tokens:\", code_tokens)\n",
    "print(\"Data Flow Graph (DFG):\", dfg)\n",
    "\n",
    "# print(\"Control Flow Graph (CFG):\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_controlflow(code, parser, lang):\n",
    "    \"\"\"\n",
    "    Extract control flow edges (CFG) from the code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the code to obtain the AST\n",
    "        tree = parser[0].parse(bytes(code, 'utf8'))\n",
    "        root_node = tree.root_node\n",
    "\n",
    "        # Extract control flow edges from the AST\n",
    "        cfg = extract_control_flow_edges(root_node)\n",
    "        return cfg\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting CFG: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_control_flow_edges(root_node):\n",
    "    \"\"\"\n",
    "    Extract control flow edges (e.g., branching, loops) from the AST.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node):\n",
    "        if node.type == \"if_statement\":\n",
    "            # Extract the condition\n",
    "            condition_node = next(\n",
    "                (child for child in node.children if child.type == \"parenthesized_expression\"), None\n",
    "            )\n",
    "            # Extract the \"then\" block\n",
    "            then_node = next(\n",
    "                (child for child in node.children if child.type == \"block\"), None\n",
    "            )\n",
    "            # Add edges for condition -> then\n",
    "            if condition_node and then_node:\n",
    "                edges.append((condition_node.start_point, then_node.start_point))\n",
    "            # Extract the \"else\" block, if present\n",
    "            else_node = next(\n",
    "                (child for child in node.children if child.type == \"else_body\"), None\n",
    "            )\n",
    "            if condition_node and else_node:\n",
    "                edges.append((condition_node.start_point, else_node.start_point))\n",
    "        elif node.type in (\"for_statement\", \"while_statement\"):\n",
    "            # Extract loop condition and body\n",
    "            condition_node = next(\n",
    "                (child for child in node.children if child.type == \"parenthesized_expression\"), None\n",
    "            )\n",
    "            body_node = next(\n",
    "                (child for child in node.children if child.type == \"block\"), None\n",
    "            )\n",
    "            if condition_node and body_node:\n",
    "                edges.append((condition_node.start_point, body_node.start_point))\n",
    "        elif node.type == \"return_statement\":\n",
    "            # Add a return statement as a flow edge\n",
    "            edges.append((node.start_point, node.end_point))\n",
    "\n",
    "        # Recurse for all children\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Flow Graph (CFG): [((3, 11), (3, 20)), ((4, 12), (4, 21)), ((6, 8), (6, 36))]\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "public class Factorial {\n",
    "    public static long factorial(int n) {\n",
    "        if (n <= 1) {\n",
    "            return 1;\n",
    "        }\n",
    "        return n * factorial(n - 1);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Extract CFG\n",
    "cfg = extract_controlflow(code, parsers[\"java\"], \"java\")\n",
    "\n",
    "print(\"Control Flow Graph (CFG):\", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    source,target=filename.split(',')\n",
    "    lang='java'\n",
    "    if source[-1]=='s':\n",
    "        lang='c_sharp'\n",
    "        \n",
    "    with open(source,encoding=\"utf-8\") as f1,open(target,encoding=\"utf-8\") as f2:\n",
    "        for line1,line2 in zip(f1,f2):\n",
    "            line1=line1.strip()\n",
    "            line2=line2.strip()\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    source=line1,\n",
    "                    target=line2,\n",
    "                    lang=lang\n",
    "                        ) \n",
    "            )\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the input for BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,                 \n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg = extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        cfg = extract_controlflow(example.source,\n",
    "                                    parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                    \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        # print(\"Code Tokens:\", code_tokens)\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))    \n",
    "        code_tokens=[y for x in code_tokens for y in x]  \n",
    "        \n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:args.max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        \n",
    "        # process dfg\n",
    "        dfg=dfg[:args.max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        \n",
    "        # process cfg\n",
    "        # Map CFG to token positions\n",
    "        cfg_mapped = []\n",
    "        for start, end in cfg:\n",
    "            if start in ori2cur_pos and end in ori2cur_pos:\n",
    "                start_pos = ori2cur_pos[start][0]\n",
    "                end_pos = ori2cur_pos[end][1]\n",
    "                cfg_mapped.append((start_pos, end_pos))\n",
    "\n",
    "        # Flatten CFG into the source structure\n",
    "        for start_pos, end_pos in cfg_mapped:\n",
    "            source_tokens.append(\"[CFG_EDGE]\")\n",
    "            position_idx.append(0)\n",
    "            source_ids.append(tokenizer.unk_token_id)  # Add an unknown token ID for CFG edge\n",
    "        \n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=args.max_source_length-len(source_ids)\n",
    "        \n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length   \n",
    "           \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length        \n",
    "        \n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)    \n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        \n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]        \n",
    "      \n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                logger.info(\"position_idx: {}\".format(position_idx))\n",
    "                logger.info(\"dfg_to_code: {}\".format(' '.join(map(str, dfg_to_code))))\n",
    "                logger.info(\"dfg_to_dfg: {}\".format(' '.join(map(str, dfg_to_dfg))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate Semantic Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: <__main__.Args object at 0x7f79bc49bcd0>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Simulate argparse for Jupyter\n",
    "class Args:\n",
    "    model_type = \"roberta\"\n",
    "    model_name_or_path = \"roberta-base\"\n",
    "    output_dir = \"./output\"\n",
    "    load_model_path = None\n",
    "    train_filename = None\n",
    "    dev_filename = None\n",
    "    test_filename = None\n",
    "    source_lang = \"en\"\n",
    "    config_name = \"\"\n",
    "    tokenizer_name = \"\"\n",
    "    max_source_length = 64\n",
    "    max_target_length = 32\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_test = False\n",
    "    do_lower_case = False\n",
    "    no_cuda = False\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 64\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 5e-5\n",
    "    beam_size = 10\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps = -1\n",
    "    eval_steps = -1\n",
    "    train_steps = -1\n",
    "    warmup_steps = 0\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "\n",
    "# Create an instance of Args\n",
    "args = Args()\n",
    "\n",
    "# Print the arguments for verification\n",
    "print(f\"Arguments: {args}\")\n",
    "args.model_type = \"roberta\"\n",
    "args.source_lang = source\n",
    "args.target_lang = target\n",
    "args.model_name_or_path = pretrained_model\n",
    "args.tokenizer_name = \"microsoft/graphcodebert-base\"\n",
    "args.config_name = \"microsoft/graphcodebert-base\"\n",
    "args.train_filename = train_file\n",
    "args.dev_filename = dev_file\n",
    "args.output_dir = output_dir\n",
    "args.learning_rate = lr\n",
    "args.num_train_epochs = epochs\n",
    "args.train_batch_size = batch_size\n",
    "args.eval_batch_size = batch_size\n",
    "args.max_source_length = source_length\n",
    "args.max_target_length = target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parsers for each language\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples, args):\n",
    "        self.examples = examples\n",
    "        self.args=args  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #calculate graph-guided masked function\n",
    "        attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool_)\n",
    "        #calculate begin index of node and max length of input\n",
    "        node_index=sum([i>1 for i in self.examples[item].position_idx])\n",
    "        max_length=sum([i!=1 for i in self.examples[item].position_idx])\n",
    "        #sequence can attend to sequence\n",
    "        attn_mask[:node_index,:node_index]=True\n",
    "        #special tokens attend to all tokens\n",
    "        for idx,i in enumerate(self.examples[item].source_ids):\n",
    "            if i in [0,2]:\n",
    "                attn_mask[idx,:max_length]=True\n",
    "        #nodes attend to code tokens that are identified from\n",
    "        for idx,(a,b) in enumerate(self.examples[item].dfg_to_code):\n",
    "            if a<node_index and b<node_index:\n",
    "                attn_mask[idx+node_index,a:b]=True\n",
    "                attn_mask[a:b,idx+node_index]=True\n",
    "        #nodes attend to adjacent nodes         \n",
    "        for idx,nodes in enumerate(self.examples[item].dfg_to_dfg):\n",
    "            for a in nodes:\n",
    "                if a+node_index<len(self.examples[item].position_idx):\n",
    "                    attn_mask[idx+node_index,a+node_index]=True  \n",
    "                    \n",
    "        return (torch.tensor(self.examples[item].source_ids),\n",
    "                torch.tensor(self.examples[item].source_mask),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask), \n",
    "                torch.tensor(self.examples[item].target_ids),\n",
    "                torch.tensor(self.examples[item].target_mask),)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA, GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dir if output_dir not exist\n",
    "if os.path.exists(args.output_dir) is False:\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir, step):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model \n",
    "    output_model_file = os.path.join(output_dir, \"model.{}.bin\".format(step))\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, dev_dataset):\n",
    "    if 'dev_bleu' in dev_dataset:\n",
    "        eval_examples,eval_data=dev_dataset['dev_bleu']\n",
    "    else:\n",
    "        eval_examples = read_examples(args.dev_filename)\n",
    "        eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "        eval_data = TextDataset(eval_features,args)\n",
    "        dev_dataset['dev_bleu']=eval_examples,eval_data\n",
    "        \n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p,eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    return dev_bleu,xmatch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, eval_examples, eval_dataloader):\n",
    "    \n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids, source_mask, position_idx, att_mask, target_ids, target_mask) \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p, eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    \n",
    "    return dev_bleu,xmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push to huggingface\n",
    "# !huggingface-cli login\n",
    "# repo_url = \"https://huggingface.co/judynguyen16/graphcodebert--code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Lightning Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------*****---------\n",
    "# # Prepare training loader and fine-tuning\n",
    "\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "# config = config_class.from_pretrained(args.config_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "# train_examples = read_examples(args.train_filename)\n",
    "# train_features = convert_examples_to_features(train_examples, tokenizer, args,stage='train')\n",
    "# train_data = TextDataset(train_features,args)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=54)\n",
    "# num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "# #Start training\n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "# logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "# logger.info(\"  Num epoch = %d\", args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 381.97it/s]\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 54 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "11/27/2024 21:23:07 - INFO - __main__ -   \n",
      "***** Running evaluation *****\n",
      "11/27/2024 21:23:07 - INFO - __main__ -     Num examples = 500\n",
      "11/27/2024 21:23:07 - INFO - __main__ -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# validation loader\n",
    "dev_dataset={}\n",
    "if 'dev_loss' in dev_dataset:\n",
    "    eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "else:\n",
    "    eval_examples = read_examples(args.dev_filename)\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
    "    eval_data = TextDataset(eval_features,args)\n",
    "    dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=54)\n",
    "\n",
    "logger.info(\"\\n***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2024 21:23:07 - INFO - __main__ -   Test file: data/valid.java-cs.txt.java,data/valid.java-cs.txt.cs\n",
      "100%|██████████| 50/50 [00:00<00:00, 546.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "files=[]\n",
    "if args.dev_filename is not None:\n",
    "    files.append(args.dev_filename)\n",
    "if args.test_filename is not None:\n",
    "    files.append(args.test_filename)\n",
    "    \n",
    "for idx,file in enumerate(files):   \n",
    "    logger.info(\"Test file: {}\".format(file))\n",
    "    eval_examples = read_examples(file)\n",
    "    eval_examples = eval_examples[:50]\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "    eval_data = TextDataset(eval_features,args) \n",
    "\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    test_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Example at 0x7f78ec6212d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_file = \"custom_data/valid.source.txt.java,custom_data/valid.target.txt.cs\"\n",
    "custom_eval_examples = read_examples(custom_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public static long factorial(int n) { if (n <= 1) { return 1; } return n * factorial(n - 1); }\n",
      "public static long Factorial(int n) => n <= 1 ? 1 : n * Factorial(n - 1);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 691.18it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_example = custom_eval_examples[0]\n",
    "print(eval_example.source)\n",
    "print(eval_example.target)\n",
    "eval_features = convert_examples_to_features(custom_eval_examples, tokenizer, args,stage='test')\n",
    "eval_data = TextDataset(eval_features,args)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=2,num_workers=16)\n",
    "eval_sampler = SequentialSampler(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LightningModule model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "#budild model\n",
    "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
    "                beam_size=args.beam_size,max_length=args.max_target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "# if args.load_model_path is not None:\n",
    "# pretrained_model_path = \"saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\"\n",
    "# print(\"reload model from {}\".format(pretrained_model_path))\n",
    "# model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10300 [00:00<?, ?it/s]11/27/2024 21:23:08 - INFO - __main__ -   *** Example ***\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Result', '_list', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_', '_(', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_ids: 0 15110 9527 29235 7529 35615 3999 35571 565 40981 48136 889 29235 7529 35615 3999 35571 565 40981 1437 36 9527 29235 7529 35615 3999 35571 565 40981 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 36583 29235 7529 35615 3999 35571 565 40981 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_code: (30, 31) (33, 34) (33, 34) (35, 39) (40, 41) (54, 55)\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '(', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_ids: 0 15110 6229 9527 29235 7529 35615 3999 35571 565 40981 47806 9527 29235 7529 35615 3999 35571 565 40981 1640 36583 29235 7529 35615 3999 35571 565 40981 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 36583 29235 7529 35615 3999 35571 565 40981 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   *** Example ***\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Update', 'J', 'ourney', 'State', 'Result', '_update', 'J', 'ourney', 'State', '_', '_(', '_Update', 'J', 'ourney', 'State', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'Update', 'J', 'ourney', 'State', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_ids: 0 15110 14686 863 37786 13360 48136 2935 863 37786 13360 1437 36 14686 863 37786 13360 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 39962 863 37786 13360 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_code: (18, 19) (21, 22) (21, 22) (23, 27) (28, 29) (38, 39)\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_Update', 'J', 'ourney', 'State', 'Response', '_Update', 'J', 'ourney', 'State', '(', 'Update', 'J', 'ourney', 'State', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'Update', 'J', 'ourney', 'State', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_ids: 0 15110 6229 14686 863 37786 13360 47806 14686 863 37786 13360 1640 39962 863 37786 13360 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 14686 863 37786 13360 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 14686 863 37786 13360 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 39962 863 37786 13360 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   *** Example ***\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_remove', 'Present', 'ation', 'Format', '_', '_(', '_)', '_{', '_remove', '1', 'st', 'Property', '_(', '_Property', 'ID', 'Map', '_.', '_PID', '_', 'PRES', 'FORM', 'AT', '_)', '_;', '_}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_ids: 0 15110 13842 3438 45195 1258 48587 1437 36 4839 25522 3438 134 620 44720 36 10491 2688 41151 479 47102 1215 29679 38036 2571 4839 25606 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_code: \n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_dfg: \n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_void', '_Remove', 'Present', 'ation', 'Format', '(){', 'M', 'utable', 'Section', '_s', '_=', '_(', 'M', 'utable', 'Section', ')', 'First', 'Section', ';', 's', '.', 'Remove', 'Property', '(', 'Property', 'ID', 'Map', '.', 'P', 'ID', '_', 'PRES', 'FORM', 'AT', ');', '}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_ids: 0 15110 13842 27336 45195 1258 48587 49215 448 41280 43480 579 5457 36 448 41280 43480 43 10993 43480 131 29 4 47583 44720 1640 44720 2688 41151 4 510 2688 1215 29679 38036 2571 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   *** Example ***\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '_(', '_int', '_first', 'Row', '_,', '_int', '_last', 'Row', '_,', '_int', '_first', 'Col', '_,', '_int', '_last', 'Col', '_)', '_{', '_this', '_(', '_)', '_;', '_add', 'Cell', 'Range', 'Address', '_(', '_first', 'Row', '_,', '_first', 'Col', '_,', '_last', 'Row', '_,', '_last', 'Col', '_)', '_;', '_}', '</s>', 'firstRow', 'lastRow', 'firstCol', 'lastCol', 'firstRow', 'firstCol', 'lastRow', 'lastCol']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_ids: 0 15110 13696 43430 46486 36583 36 6979 78 43277 2156 6979 94 43277 2156 6979 78 18551 2156 6979 94 18551 4839 25522 42 36 4839 25606 1606 40216 43430 46486 36 78 43277 2156 78 18551 2156 94 43277 2156 94 18551 4839 25606 35524 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_code: (8, 10) (12, 14) (16, 18) (20, 22) (33, 35) (36, 38) (39, 41) (42, 44)\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_dfg: [] [] [] [] [0] [2] [1] [3]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '(', 'int', '_first', 'Row', ',', '_int', '_last', 'Row', ',', '_int', '_first', 'Col', ',', '_int', '_last', 'Col', '):', '_this', '(){', 'Add', 'Cell', 'Range', 'Address', '(', 'first', 'Row', ',', '_first', 'Col', ',', '_last', 'Row', ',', '_last', 'Col', ');', '}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_ids: 0 15110 13696 43430 46486 36583 1640 2544 78 43277 6 6979 94 43277 6 6979 78 18551 6 6979 94 18551 3256 42 49215 20763 40216 43430 46486 1640 9502 43277 6 78 18551 6 94 43277 6 94 18551 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   *** Example ***\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_delete', '_', '_(', '_int', '_key', '_)', '_{', '_int', '_i', '_=', '_binary', 'Search', '_(', '_m', 'Keys', '_,', '_0', '_,', '_m', 'Size', '_,', '_key', '_)', '_;', '_if', '_(', '_i', '_>=', '_0', '_)', '_{', '_if', '_(', '_m', 'Values', '_[', '_i', '_]', '_!=', '_DE', 'LET', 'ED', '_)', '_{', '_m', 'Values', '_[', '_i', '_]', '_=', '_DE', 'LET', 'ED', '_;', '_m', 'Gar', 'bage', '_=', '_true', '_;', '_}', '_}', '_}', '</s>', 'key', 'i', 'i', 'i', 'i', 'i', 'binarySearch', 'mKeys', '0', 'mSize', 'key', 'i', 'i', 'DELETED', 'mValues', 'i', 'DELETED']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_ids: 0 15110 13842 17462 1437 36 6979 762 4839 25522 6979 939 5457 32771 39954 36 475 44534 2156 321 2156 475 45698 2156 762 4839 25606 114 36 939 49095 321 4839 25522 114 36 475 48738 646 939 27779 49333 5885 24258 1691 4839 25522 475 48738 646 939 27779 5457 5885 24258 1691 25606 475 31626 36772 5457 1528 25606 35524 35524 35524 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_code: (7, 8) (11, 12) (11, 12) (11, 12) (11, 12) (11, 12) (13, 15) (16, 18) (19, 20) (21, 23) (24, 25) (29, 30) (39, 40) (42, 45) (47, 49) (50, 51) (53, 56)\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   dfg_to_dfg: [] [6] [7] [8] [9] [10] [] [] [] [] [0] [5] [5] [] [16] [16] [13]\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_void', '_delete', '(', 'int', '_key', '){', 'int', '_i', '_=', '_binary', 'Search', '(', 'm', 'Keys', ',', '_0', ',', '_m', 'Size', ',', '_key', ');', 'if', '_(', 'i', '_>=', '_0', '){', 'if', '_(', 'm', 'Values', '[', 'i', ']', '_!=', '_DE', 'LET', 'ED', '){', 'm', 'Values', '[', 'i', ']', '_=', '_DE', 'LET', 'ED', ';', 'm', 'Gar', 'bage', '_=', '_true', ';', '}}}', '</s>']\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_ids: 0 15110 6229 13842 17462 1640 2544 762 48512 2544 939 5457 32771 39954 1640 119 44534 6 321 6 475 45698 6 762 4397 1594 36 118 49095 321 48512 1594 36 119 48738 10975 118 742 49333 5885 24258 1691 48512 119 48738 10975 118 742 5457 5885 24258 1691 131 119 31626 36772 5457 1528 131 49908 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/27/2024 21:23:08 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "100%|██████████| 10300/10300 [00:23<00:00, 441.78it/s]\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "11/27/2024 21:23:32 - INFO - __main__ -   ***** Running training *****\n",
      "11/27/2024 21:23:32 - INFO - __main__ -     Num examples = 10300\n",
      "11/27/2024 21:23:32 - INFO - __main__ -     Batch size = 64\n",
      "11/27/2024 21:23:32 - INFO - __main__ -     Num epoch = 2\n",
      "  0%|          | 0/161 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 274.44 MiB is free. Process 1804376 has 1.66 GiB memory in use. Process 1805973 has 1.45 GiB memory in use. Including non-PyTorch memory, this process has 18.59 GiB memory in use. Of the allocated memory 18.20 GiB is allocated by PyTorch, and 98.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     34\u001b[0m source_ids,source_mask,position_idx,att_mask,target_ids,target_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 35\u001b[0m loss,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43matt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# mean() to average on multi-gpu.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/dlami/nvme/judy/transformer-code-translation/GraphCodeBERT/translation/model.py:64\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source_ids, source_mask, position_idx, attn_mask, target_ids, target_mask, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m avg_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc,acd->abd\u001b[39m\u001b[38;5;124m\"\u001b[39m,nodes_to_token_mask,inputs_embeddings)\n\u001b[1;32m     62\u001b[0m inputs_embeddings\u001b[38;5;241m=\u001b[39minputs_embeddings\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m~\u001b[39mnodes_mask)[:,:,\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m+\u001b[39mavg_embeddings\u001b[38;5;241m*\u001b[39mnodes_mask[:,:,\u001b[38;5;28;01mNone\u001b[39;00m]  \n\u001b[0;32m---> 64\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#source_mask=token_mask.float()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:270\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 274.44 MiB is free. Process 1804376 has 1.66 GiB memory in use. Process 1805973 has 1.45 GiB memory in use. Including non-PyTorch memory, this process has 18.59 GiB memory in use. Of the allocated memory 18.20 GiB is allocated by PyTorch, and 98.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    # Prepare training data loader\n",
    "    train_examples = read_examples(args.train_filename)\n",
    "    train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train')\n",
    "    train_data = TextDataset(train_features,args)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=4)\n",
    "\n",
    "    num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataloader)*args.num_train_epochs*0.1,num_training_steps=len(train_dataloader)*args.num_train_epochs)\n",
    "\n",
    "    #Start training\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num epoch = %d\", args.num_train_epochs)\n",
    "    \n",
    "    model.train()\n",
    "    dev_dataset={}\n",
    "    nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        for batch in bar:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "            loss,_,_ = model(source_ids,source_mask,position_idx,att_mask,target_ids,target_mask)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "            tr_loss += loss.item()\n",
    "            train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
    "            nb_tr_examples += source_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            loss.backward()\n",
    "\n",
    "            if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
    "                #Update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "\n",
    "        if args.do_eval and epoch in [ int(args.num_train_epochs*(i+1)//20) for i in range(20)]:\n",
    "                                                                    #Eval model with dev dataset\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0                     \n",
    "            eval_flag=False    \n",
    "            if 'dev_loss' in dev_dataset:\n",
    "                eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "            else:\n",
    "                eval_examples = read_examples(args.dev_filename)\n",
    "                eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
    "                eval_data = TextDataset(eval_features,args)\n",
    "                dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "\n",
    "            logger.info(\"\\n***** Running evaluation *****\")\n",
    "            logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "            logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "            #Start Evaling model\n",
    "            model.eval()\n",
    "            eval_loss,tokens_num = 0,0\n",
    "            for batch in eval_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)               \n",
    "                source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "                with torch.no_grad():\n",
    "                    _,loss,num = model(source_ids,source_mask,position_idx,att_mask,target_ids,target_mask)     \n",
    "                eval_loss += loss.sum().item()\n",
    "                tokens_num += num.sum().item()\n",
    "            #Pring loss of dev dataset    \n",
    "            model.train()\n",
    "            eval_loss = eval_loss / tokens_num\n",
    "            result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
    "                        'global_step': global_step+1,\n",
    "                        'train_loss': round(train_loss,5)}\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            logger.info(\"  \"+\"*\"*20)   \n",
    "\n",
    "            #save last checkpoint\n",
    "            last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "            if not os.path.exists(last_output_dir):\n",
    "                os.makedirs(last_output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)                    \n",
    "            if eval_loss<best_loss:\n",
    "                logger.info(\"  Best ppl:%s\",round(np.exp(eval_loss),5))\n",
    "                logger.info(\"  \"+\"*\"*20)\n",
    "                best_loss=eval_loss\n",
    "                # Save best checkpoint for best ppl\n",
    "                output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "                output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)  \n",
    "\n",
    "\n",
    "            #Calculate bleu  \n",
    "            if 'dev_bleu' in dev_dataset:\n",
    "                eval_examples,eval_data=dev_dataset['dev_bleu']\n",
    "            else:\n",
    "                eval_examples = read_examples(args.dev_filename)\n",
    "                eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
    "                eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "                eval_data = TextDataset(eval_features,args)\n",
    "                dev_dataset['dev_bleu']=eval_examples,eval_data\n",
    "\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "            model.eval() \n",
    "            p=[]\n",
    "            for batch in eval_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "                with torch.no_grad():\n",
    "                    preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "                    for pred in preds:\n",
    "                        t=pred[0].cpu().numpy()\n",
    "                        t=list(t)\n",
    "                        if 0 in t:\n",
    "                            t=t[:t.index(0)]\n",
    "                        text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                        p.append(text)\n",
    "            model.train()\n",
    "            predictions=[]\n",
    "            accs=[]\n",
    "            with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "                for ref,gold in zip(p,eval_examples):\n",
    "                    predictions.append(ref)\n",
    "                    f.write(ref+'\\n')\n",
    "                    f1.write(gold.target+'\\n')     \n",
    "                    accs.append(ref==gold.target)\n",
    "\n",
    "            dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "            xmatch=round(np.mean(accs)*100,4)\n",
    "            logger.info(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
    "            logger.info(\"  %s = %s \"%(\"xMatch\",str(round(np.mean(accs)*100,4))))\n",
    "            logger.info(\"  \"+\"*\"*20)    \n",
    "            if dev_bleu+xmatch>best_bleu:\n",
    "                logger.info(\"  Best BLEU+xMatch:%s\",dev_bleu+xmatch)\n",
    "                logger.info(\"  \"+\"*\"*20)\n",
    "                best_bleu=dev_bleu+xmatch\n",
    "                # Save best checkpoint for best bleu\n",
    "                output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "                output_model_file = os.path.join(output_dir, \"pytorch_model_ast.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    model.eval() \n",
    "    model.to(device)\n",
    "    p=[]\n",
    "    batch = next(iter(dataloader))\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch         \n",
    "          \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids,source_mask,position_idx,att_mask, None, None)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "np.bool = np.bool_\n",
    "p=[]\n",
    "for batch in tqdm(test_dataloader,total=len(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                    \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids,source_mask,position_idx,att_mask, None, None)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "accs = []\n",
    "with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "    for ref, gold in zip(p, eval_examples):\n",
    "        predictions.append(ref)\n",
    "        f.write(ref+'\\n')\n",
    "        f1.write(gold.target+'\\n')     \n",
    "        accs.append(ref==gold.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "xmatch=round(np.mean(accs)*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {dev_bleu}, X-Match: {xmatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login huggerface\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id = \"judynguyen16/graphcodebert-code-translation-java-cs\"\n",
    "# !huggingface-cli repo create \"judynguyen16/graphcodebert-code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "# Push model and tokenizer to Hugging Face Hub\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "repo_id = \"judynguyen16/graphcodebert-code-translation-java-cs\"\n",
    "\n",
    "files_to_upload = {\n",
    "    \"pytorch_model.bin\": \"/home/ubuntu/judy/transformer-code-translation/GraphCodeBERT/translation/saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\",\n",
    "}\n",
    "\n",
    "for dest_path, local_path in files_to_upload.items():\n",
    "    upload_file(\n",
    "        path_or_fileobj=local_path,\n",
    "        path_in_repo=dest_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the file\n",
    "url = \"https://huggingface.co/judynguyen16/graphcodebert-code-translation-java-cs/resolve/main/pytorch_model.bin\"\n",
    "\n",
    "# Path to save the file\n",
    "save_path = \"pytorch_model.bin\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"File downloaded and saved to {save_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
