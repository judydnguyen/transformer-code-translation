{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"java\"\n",
    "target=\"cs\"\n",
    "lr=1e-4\n",
    "batch_size=64\n",
    "beam_size=10\n",
    "source_length=320\n",
    "target_length=256\n",
    "output_dir=f\"saved_models/{source}-{target}/\"\n",
    "train_file=f\"data/train.java-cs.txt.{source},data/train.java-cs.txt.{target}\"\n",
    "dev_file=f\"data/valid.java-cs.txt.{source},data/valid.java-cs.txt.{target}\"\n",
    "epochs=2\n",
    "pretrained_model=\"microsoft/graphcodebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dfg_function={\n",
    "    'python': DFG_python,\n",
    "    'java': DFG_java,\n",
    "    'ruby': DFG_ruby,\n",
    "    'go': DFG_go,\n",
    "    'php': DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#load parsers\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    # print(Language)\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 source,\n",
    "                 target,\n",
    "                 lang\n",
    "                 ):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lang=lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass    \n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"    \n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "        root_node = tree.root_node  \n",
    "        tokens_index=tree_to_token_index(root_node)     \n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]  \n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)  \n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{}) \n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    source,target=filename.split(',')\n",
    "    lang='java'\n",
    "    if source[-1]=='s':\n",
    "        lang='c_sharp'\n",
    "        \n",
    "    with open(source,encoding=\"utf-8\") as f1,open(target,encoding=\"utf-8\") as f2:\n",
    "        for line1,line2 in zip(f1,f2):\n",
    "            line1=line1.strip()\n",
    "            line2=line2.strip()\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    source=line1,\n",
    "                    target=line2,\n",
    "                    lang=lang\n",
    "                        ) \n",
    "            )\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the input for BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,                 \n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg=extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))    \n",
    "        code_tokens=[y for x in code_tokens for y in x]  \n",
    "        \n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:args.max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        dfg=dfg[:args.max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=args.max_source_length-len(source_ids)\n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length      \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length        \n",
    "        \n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)    \n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]        \n",
    "      \n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                logger.info(\"position_idx: {}\".format(position_idx))\n",
    "                logger.info(\"dfg_to_code: {}\".format(' '.join(map(str, dfg_to_code))))\n",
    "                logger.info(\"dfg_to_dfg: {}\".format(' '.join(map(str, dfg_to_dfg))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: <__main__.Args object at 0x7f7340f9f790>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Simulate argparse for Jupyter\n",
    "class Args:\n",
    "    model_type = \"roberta\"\n",
    "    model_name_or_path = \"roberta-base\"\n",
    "    output_dir = \"./output\"\n",
    "    load_model_path = None\n",
    "    train_filename = None\n",
    "    dev_filename = None\n",
    "    test_filename = None\n",
    "    source_lang = \"en\"\n",
    "    config_name = \"\"\n",
    "    tokenizer_name = \"\"\n",
    "    max_source_length = 64\n",
    "    max_target_length = 32\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_test = False\n",
    "    do_lower_case = False\n",
    "    no_cuda = False\n",
    "    train_batch_size = 256\n",
    "    eval_batch_size = 512\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 5e-5\n",
    "    beam_size = 10\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps = -1\n",
    "    eval_steps = -1\n",
    "    train_steps = -1\n",
    "    warmup_steps = 0\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "\n",
    "# Create an instance of Args\n",
    "args = Args()\n",
    "\n",
    "# Print the arguments for verification\n",
    "print(f\"Arguments: {args}\")\n",
    "args.model_type = \"roberta\"\n",
    "args.source_lang = source\n",
    "args.target_lang = target\n",
    "args.model_name_or_path = pretrained_model\n",
    "args.tokenizer_name = \"microsoft/graphcodebert-base\"\n",
    "args.config_name = \"microsoft/graphcodebert-base\"\n",
    "args.train_filename = train_file\n",
    "args.dev_filename = dev_file\n",
    "args.output_dir = output_dir\n",
    "args.learning_rate = lr\n",
    "args.num_train_epochs = epochs\n",
    "args.train_batch_size = batch_size\n",
    "args.eval_batch_size = batch_size\n",
    "args.max_source_length = source_length\n",
    "args.max_target_length = target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parsers for each language\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples, args):\n",
    "        self.examples = examples\n",
    "        self.args=args  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #calculate graph-guided masked function\n",
    "        attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool_)\n",
    "        #calculate begin index of node and max length of input\n",
    "        node_index=sum([i>1 for i in self.examples[item].position_idx])\n",
    "        max_length=sum([i!=1 for i in self.examples[item].position_idx])\n",
    "        #sequence can attend to sequence\n",
    "        attn_mask[:node_index,:node_index]=True\n",
    "        #special tokens attend to all tokens\n",
    "        for idx,i in enumerate(self.examples[item].source_ids):\n",
    "            if i in [0,2]:\n",
    "                attn_mask[idx,:max_length]=True\n",
    "        #nodes attend to code tokens that are identified from\n",
    "        for idx,(a,b) in enumerate(self.examples[item].dfg_to_code):\n",
    "            if a<node_index and b<node_index:\n",
    "                attn_mask[idx+node_index,a:b]=True\n",
    "                attn_mask[a:b,idx+node_index]=True\n",
    "        #nodes attend to adjacent nodes         \n",
    "        for idx,nodes in enumerate(self.examples[item].dfg_to_dfg):\n",
    "            for a in nodes:\n",
    "                if a+node_index<len(self.examples[item].position_idx):\n",
    "                    attn_mask[idx+node_index,a+node_index]=True  \n",
    "                    \n",
    "        return (torch.tensor(self.examples[item].source_ids),\n",
    "                torch.tensor(self.examples[item].source_mask),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask), \n",
    "                torch.tensor(self.examples[item].target_ids),\n",
    "                torch.tensor(self.examples[item].target_mask),)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA, GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dir if output_dir not exist\n",
    "if os.path.exists(args.output_dir) is False:\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir, step):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model \n",
    "    output_model_file = os.path.join(output_dir, \"model.{}.bin\".format(step))\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, dev_dataset):\n",
    "    if 'dev_bleu' in dev_dataset:\n",
    "        eval_examples,eval_data=dev_dataset['dev_bleu']\n",
    "    else:\n",
    "        eval_examples = read_examples(args.dev_filename)\n",
    "        eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "        eval_data = TextDataset(eval_features,args)\n",
    "        dev_dataset['dev_bleu']=eval_examples,eval_data\n",
    "        \n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p,eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    return dev_bleu,xmatch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, eval_examples, eval_dataloader):\n",
    "    \n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids, source_mask, position_idx, att_mask, target_ids, target_mask) \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p, eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    \n",
    "    return dev_bleu,xmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push to huggingface\n",
    "# !huggingface-cli login\n",
    "# repo_url = \"https://huggingface.co/judynguyen16/graphcodebert--code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Lightning Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------*****---------\n",
    "# # Prepare training loader and fine-tuning\n",
    "\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "# config = config_class.from_pretrained(args.config_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "# train_examples = read_examples(args.train_filename)\n",
    "# train_features = convert_examples_to_features(train_examples, tokenizer, args,stage='train')\n",
    "# train_data = TextDataset(train_features,args)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=54)\n",
    "# num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "# #Start training\n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "# logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "# logger.info(\"  Num epoch = %d\", args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 378.98it/s]\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 54 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "11/27/2024 20:33:36 - INFO - __main__ -   \n",
      "***** Running evaluation *****\n",
      "11/27/2024 20:33:36 - INFO - __main__ -     Num examples = 500\n",
      "11/27/2024 20:33:36 - INFO - __main__ -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# validation loader\n",
    "dev_dataset={}\n",
    "if 'dev_loss' in dev_dataset:\n",
    "    eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "else:\n",
    "    eval_examples = read_examples(args.dev_filename)\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
    "    eval_data = TextDataset(eval_features,args)\n",
    "    dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=54)\n",
    "\n",
    "logger.info(\"\\n***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2024 20:33:36 - INFO - __main__ -   Test file: data/valid.java-cs.txt.java,data/valid.java-cs.txt.cs\n",
      "100%|██████████| 50/50 [00:00<00:00, 703.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "files=[]\n",
    "if args.dev_filename is not None:\n",
    "    files.append(args.dev_filename)\n",
    "if args.test_filename is not None:\n",
    "    files.append(args.test_filename)\n",
    "    \n",
    "for idx,file in enumerate(files):   \n",
    "    logger.info(\"Test file: {}\".format(file))\n",
    "    eval_examples = read_examples(file)\n",
    "    eval_examples = eval_examples[:50]\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "    eval_data = TextDataset(eval_features,args) \n",
    "\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    test_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LightningModule model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload model from saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "#budild model\n",
    "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
    "                beam_size=args.beam_size,max_length=args.max_target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "# if args.load_model_path is not None:\n",
    "pretrained_model_path = \"saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\"\n",
    "print(\"reload model from {}\".format(pretrained_model_path))\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# np.bool = np.bool_\n",
    "# model.eval()\n",
    "# dev_bleu,xmatch=calculate_bleu(model, eval_examples, test_dataloader)\n",
    "# print(f\"BLEU: {dev_bleu}, X-Match: {xmatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() \n",
    "# p=[]\n",
    "# for batch in tqdm(test_dataloader,total=len(test_dataloader)):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                    \n",
    "#     with torch.no_grad():\n",
    "#         preds = model(source_ids,source_mask,position_idx,att_mask, None, None)  \n",
    "#         for pred in preds:\n",
    "#             t=pred[0].cpu().numpy()\n",
    "#             t=list(t)\n",
    "#             if 0 in t:\n",
    "#                 t=t[:t.index(0)]\n",
    "#             text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "#             p.append(text)\n",
    "            \n",
    "# # model.train()\n",
    "# predictions=[]\n",
    "# accs = []\n",
    "# with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "#     for ref, gold in zip(p, eval_examples):\n",
    "#         predictions.append(ref)\n",
    "#         f.write(ref+'\\n')\n",
    "#         f1.write(gold.target+'\\n')     \n",
    "#         accs.append(ref==gold.target)\n",
    "\n",
    "# dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "# xmatch=round(np.mean(accs)*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# def compute_score(pred, target):\n",
    "#     \"\"\"\n",
    "#     Compute BLEU score between the predicted and target text.\n",
    "#     \"\"\"\n",
    "#     reference = [target.split()]  # BLEU expects a list of references\n",
    "#     hypothesis = pred.split()\n",
    "#     return sentence_bleu(reference, hypothesis)\n",
    "\n",
    "# model.eval()\n",
    "# p = []\n",
    "\n",
    "# # Collect predictions\n",
    "# for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     source_ids, source_mask, position_idx, att_mask, target_ids, target_mask = batch\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(source_ids, source_mask, position_idx, att_mask, None, None)\n",
    "#         for pred in preds:\n",
    "#             t = pred[0].cpu().numpy()\n",
    "#             t = list(t)\n",
    "#             if 0 in t:\n",
    "#                 t = t[:t.index(0)]\n",
    "#             text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "#             p.append(text)\n",
    "\n",
    "# # Store predictions, targets, and scores\n",
    "# samples = []\n",
    "# with open(os.path.join(args.output_dir, \"dev.output\"), 'w') as f, open(os.path.join(args.output_dir, \"dev.gold\"), 'w') as f1:\n",
    "#     for ref, gold in zip(p, eval_examples):\n",
    "#         # Compute similarity score\n",
    "#         score = compute_score(ref, gold.target)\n",
    "\n",
    "#         # Store sample for further analysis\n",
    "#         samples.append({\n",
    "#             \"prediction\": ref,\n",
    "#             \"target\": gold.target,\n",
    "#             \"score\": score\n",
    "#         })\n",
    "\n",
    "#         # Save predictions and targets\n",
    "#         f.write(ref + '\\n')\n",
    "#         f1.write(gold.target + '\\n')\n",
    "\n",
    "# # Sort samples by score (ascending, lowest first)\n",
    "# samples = sorted(samples, key=lambda x: x[\"score\"])\n",
    "\n",
    "# # Save the lowest scoring samples for further analysis\n",
    "# lowest_samples_file = os.path.join(args.output_dir, \"lowest_scoring_samples.json\")\n",
    "# with open(lowest_samples_file, 'w') as f:\n",
    "#     import json\n",
    "#     json.dump(samples[:10], f, indent=4)  # Save the bottom 10 samples\n",
    "\n",
    "# print(f\"Lowest scoring samples saved to {lowest_samples_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dev_bleu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(_bleu(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.gold\u001b[39m\u001b[38;5;124m\"\u001b[39m), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.output\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m xmatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43maccs\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "xmatch=round(np.mean(accs)*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {dev_bleu}, X-Match: {xmatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login huggerface\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id = \"judynguyen16/graphcodebert-code-translation-java-cs\"\n",
    "# !huggingface-cli repo create \"judynguyen16/graphcodebert-code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# # Push model and tokenizer to Hugging Face Hub\n",
    "# from huggingface_hub import upload_file\n",
    "\n",
    "# repo_id = \"judynguyen16/graphcodebert-code-translation-java-cs\"\n",
    "\n",
    "# files_to_upload = {\n",
    "#     \"pytorch_model.bin\": \"/home/ubuntu/judy/transformer-code-translation/GraphCodeBERT/translation/saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\",\n",
    "# }\n",
    "\n",
    "# for dest_path, local_path in files_to_upload.items():\n",
    "#     upload_file(\n",
    "#         path_or_fileobj=local_path,\n",
    "#         path_in_repo=dest_path,\n",
    "#         repo_id=repo_id,\n",
    "#         repo_type=\"model\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get failed samples for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public static long factorial(int n) { long result = 1; for (int i = 2; i <= n; i++) { result *= i; } return result; }\n",
      "public static long Factorial(int n) { long result = 1; for (int i = 2; i <= n; i++) { result *= i; } return result; }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 663.06it/s]\n"
     ]
    }
   ],
   "source": [
    "custom_file = \"custom_data/valid.source.2.txt.java,custom_data/valid.target.2.txt.cs\"\n",
    "custom_eval_examples = read_examples(custom_file)\n",
    "eval_example = custom_eval_examples[2]\n",
    "print(eval_example.source)\n",
    "print(eval_example.target)\n",
    "eval_features = convert_examples_to_features(custom_eval_examples, tokenizer, args,stage='test')\n",
    "eval_data = TextDataset(eval_features,args)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=20,num_workers=16)\n",
    "eval_sampler = SequentialSampler(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    model.eval() \n",
    "    model.to(device)\n",
    "    p=[]\n",
    "    batch = next(iter(dataloader))\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch         \n",
    "          \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids,source_mask,position_idx,att_mask, None, None)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_predictions(model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public static void Main(string[] args){#pragma warning disable 612, 618}}',\n",
       " 'public static long Factorial(int n){if (n <= 1){return 1;}return n * Factorial(n - 1);}',\n",
       " 'public static long Factorial(int n){long result = 1;for (int i = 2; i <= n; i++){result *= i;}return result;}',\n",
       " 'public static int Add(int a, int b){return a + b;}',\n",
       " 'public static int Subtract(int a, int b){return a - b;}',\n",
       " 'public static void printNumbers(int n){for (int i = 0; i < n; i++){Output.WriteLine(i);}}',\n",
       " 'public static bool IsEven(int n){return n % 2 == 0;}',\n",
       " 'public static string GetDay(int date){switch (this){case 1:return \"Monday\";case 2:return \"Monday\";case 2:return \"Monday\";}}',\n",
       " 'public static void divide(int a, int b){try{Output.WriteLine(a / b);}catch (ArgumentException){Output.WriteLine(\"Cannot divide by zero\");}}',\n",
       " 'public static void printEvenNumbers(int n){for (int i = 0; i <= n; i++){if (i % 2 == 0){Output.WriteLine(i);}}}',\n",
       " 'public static void CheckNumber(int x){if (x > 0){if (x < 10){Console.WriteLine(\"Between 1 and 9\");}else{Output.WriteLine(\"10 or higher\");}}else{#pragma warning restore 612, 618}}',\n",
       " 'public static string ReverseString(string input){return new StringBuilder(input).ToString();}',\n",
       " 'public static bool IsPalindrome(String input){string reverted = new StringBuilder(input).reverse().ToString();return input.Equals(removed);}',\n",
       " 'public static int SumArray(int[] arr){int sum = 0;foreach (int num in arr){sum += num;}return sum;}',\n",
       " 'public static int FindIndex(int[] arr, int target){for (int i = 0; i < arr.Length; i++){if (arr[i] == target){return i;}}return -1;}',\n",
       " 'public static void BlubSort(int[] arr){for (int i = 0; i < arr.Length - 1; i++){for (int j = 0; j < arr.Length - i - 1; j++){if (arr[j] > arr[j + 1]){int temp = arr[j];arr[j] = arr[j + 1];arr[j + 1] = temp;}}}',\n",
       " 'public static bool IsPrime(int n){if (n < 2){return false;}for (int i = 2; i <= Math.Sqrt(n); i++){if (n % i == 0){return false;}}return true;}',\n",
       " 'public static int[] fibonacci(int n){int[] fib = new int[n];ref[0] = 0;if (n > 1){buff[1] = 1;}for (int i = 2; i < n; i++){long[i] = br[i - 1];}return fib;}',\n",
       " 'public static int[] MulMath(int[] a, int[] b){int rows = a.Length;int cols = b[0].Length;int[] result = new int[rows][];for (int i = 0; i < rows; i++){for (int j = 0; j < rows; j++){result[i] += a[i][k] * b[k];}}}return result;}',\n",
       " 'public static string ReadFile(string path){try{return new string(IO.ReadAllBytes(path));}catch (IOException){return string.Empty;}}']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # URL of the file\n",
    "# url = \"https://huggingface.co/judynguyen16/graphcodebert-code-translation-java-cs/resolve/main/pytorch_model.bin\"\n",
    "\n",
    "# # Path to save the file\n",
    "# save_path = \"pytorch_model.bin\"\n",
    "\n",
    "# # Download the file\n",
    "# response = requests.get(url, stream=True)\n",
    "# if response.status_code == 200:\n",
    "#     with open(save_path, \"wb\") as f:\n",
    "#         for chunk in response.iter_content(chunk_size=8192):\n",
    "#             f.write(chunk)\n",
    "#     print(f\"File downloaded and saved to {save_path}\")\n",
    "# else:\n",
    "#     print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
