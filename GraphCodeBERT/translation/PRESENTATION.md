## Semantic-enhanced Code Translation with Transformer-based Models
------------
*Authors: Dung (Judy) Nguyen, Samuel Sasaki* 
Email: {dung.t.nguyen, samuel.sasaki}@vanderbilt.edu

### Overview
Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. 
However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process.
> *A code snippet contains more than a sequence of tokens!!!*


#### GraphCodeBert

![Data Flow Graph](/home/ubuntu/judy/transformer-code-translation/GraphCodeBERT/translation/assets/dfg_graphcodebert.png)


- Study the Transformer-based Models in code understanding task: Code Translation.
- Observation: Using Data Control Flow Graph is efficient but still not enough.
- Conclusion: Incoporating semantic information (Control Flow Graph) can help improve model performance.

### Our Approach

### Model card/dataset card

### Critical Analysis

### Resource links
- Papers:
- Code:

### Code demonstration

