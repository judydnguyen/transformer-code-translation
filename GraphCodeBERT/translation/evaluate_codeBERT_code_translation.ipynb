{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"java\"\n",
    "target=\"cs\"\n",
    "lr=1e-4\n",
    "batch_size=64\n",
    "beam_size=10\n",
    "source_length=320\n",
    "target_length=256\n",
    "output_dir=f\"saved_models/{source}-{target}/\"\n",
    "train_file=f\"data/train.java-cs.txt.{source},data/train.java-cs.txt.{target}\"\n",
    "dev_file=f\"data/valid.java-cs.txt.{source},data/valid.java-cs.txt.{target}\"\n",
    "epochs=2\n",
    "pretrained_model=\"microsoft/graphcodebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'ruby':DFG_ruby,\n",
    "    'go':DFG_go,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#load parsers\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    # print(Language)\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 source,\n",
    "                 target,\n",
    "                 lang\n",
    "                 ):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lang=lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass    \n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"    \n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "        root_node = tree.root_node  \n",
    "        tokens_index=tree_to_token_index(root_node)     \n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]  \n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)  \n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{}) \n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    source,target=filename.split(',')\n",
    "    lang='java'\n",
    "    if source[-1]=='s':\n",
    "        lang='c_sharp'\n",
    "        \n",
    "    with open(source,encoding=\"utf-8\") as f1,open(target,encoding=\"utf-8\") as f2:\n",
    "        for line1,line2 in zip(f1,f2):\n",
    "            line1=line1.strip()\n",
    "            line2=line2.strip()\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    source=line1,\n",
    "                    target=line2,\n",
    "                    lang=lang\n",
    "                        ) \n",
    "            )\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the input for BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,                 \n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg=extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))    \n",
    "        code_tokens=[y for x in code_tokens for y in x]  \n",
    "        \n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:args.max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        dfg=dfg[:args.max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=args.max_source_length-len(source_ids)\n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length      \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length        \n",
    "        \n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)    \n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]        \n",
    "      \n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                logger.info(\"position_idx: {}\".format(position_idx))\n",
    "                logger.info(\"dfg_to_code: {}\".format(' '.join(map(str, dfg_to_code))))\n",
    "                logger.info(\"dfg_to_dfg: {}\".format(' '.join(map(str, dfg_to_dfg))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: <__main__.Args object at 0x7fc0e7450910>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Simulate argparse for Jupyter\n",
    "class Args:\n",
    "    model_type = \"roberta\"\n",
    "    model_name_or_path = \"roberta-base\"\n",
    "    output_dir = \"./output\"\n",
    "    load_model_path = None\n",
    "    train_filename = None\n",
    "    dev_filename = None\n",
    "    test_filename = None\n",
    "    source_lang = \"en\"\n",
    "    config_name = \"\"\n",
    "    tokenizer_name = \"\"\n",
    "    max_source_length = 64\n",
    "    max_target_length = 32\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_test = False\n",
    "    do_lower_case = False\n",
    "    no_cuda = False\n",
    "    train_batch_size = 256\n",
    "    eval_batch_size = 512\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 5e-5\n",
    "    beam_size = 10\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps = -1\n",
    "    eval_steps = -1\n",
    "    train_steps = -1\n",
    "    warmup_steps = 0\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "\n",
    "# Create an instance of Args\n",
    "args = Args()\n",
    "\n",
    "# Print the arguments for verification\n",
    "print(f\"Arguments: {args}\")\n",
    "args.model_type = \"roberta\"\n",
    "args.source_lang = source\n",
    "args.target_lang = target\n",
    "args.model_name_or_path = pretrained_model\n",
    "args.tokenizer_name = \"microsoft/graphcodebert-base\"\n",
    "args.config_name = \"microsoft/graphcodebert-base\"\n",
    "args.train_filename = train_file\n",
    "args.dev_filename = dev_file\n",
    "args.output_dir = output_dir\n",
    "args.learning_rate = lr\n",
    "args.num_train_epochs = epochs\n",
    "args.train_batch_size = batch_size\n",
    "args.eval_batch_size = batch_size\n",
    "args.max_source_length = source_length\n",
    "args.max_target_length = target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parsers for each language\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples, args):\n",
    "        self.examples = examples\n",
    "        self.args=args  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #calculate graph-guided masked function\n",
    "        attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)\n",
    "        #calculate begin index of node and max length of input\n",
    "        node_index=sum([i>1 for i in self.examples[item].position_idx])\n",
    "        max_length=sum([i!=1 for i in self.examples[item].position_idx])\n",
    "        #sequence can attend to sequence\n",
    "        attn_mask[:node_index,:node_index]=True\n",
    "        #special tokens attend to all tokens\n",
    "        for idx,i in enumerate(self.examples[item].source_ids):\n",
    "            if i in [0,2]:\n",
    "                attn_mask[idx,:max_length]=True\n",
    "        #nodes attend to code tokens that are identified from\n",
    "        for idx,(a,b) in enumerate(self.examples[item].dfg_to_code):\n",
    "            if a<node_index and b<node_index:\n",
    "                attn_mask[idx+node_index,a:b]=True\n",
    "                attn_mask[a:b,idx+node_index]=True\n",
    "        #nodes attend to adjacent nodes         \n",
    "        for idx,nodes in enumerate(self.examples[item].dfg_to_dfg):\n",
    "            for a in nodes:\n",
    "                if a+node_index<len(self.examples[item].position_idx):\n",
    "                    attn_mask[idx+node_index,a+node_index]=True  \n",
    "                    \n",
    "        return (torch.tensor(self.examples[item].source_ids),\n",
    "                torch.tensor(self.examples[item].source_mask),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask), \n",
    "                torch.tensor(self.examples[item].target_ids),\n",
    "                torch.tensor(self.examples[item].target_mask),)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA, GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dir if output_dir not exist\n",
    "if os.path.exists(args.output_dir) is False:\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir, step):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model \n",
    "    output_model_file = os.path.join(output_dir, \"model.{}.bin\".format(step))\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, dev_dataset):\n",
    "    if 'dev_bleu' in dev_dataset:\n",
    "        eval_examples,eval_data=dev_dataset['dev_bleu']\n",
    "    else:\n",
    "        eval_examples = read_examples(args.dev_filename)\n",
    "        eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "        eval_data = TextDataset(eval_features,args)\n",
    "        dev_dataset['dev_bleu']=eval_examples,eval_data\n",
    "        \n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p,eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    return dev_bleu,xmatch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, eval_examples, eval_dataloader):\n",
    "    \n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids, source_mask, position_idx, att_mask, target_ids, target_mask) \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p, eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    \n",
    "    return dev_bleu,xmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push to huggingface\n",
    "# !huggingface-cli login\n",
    "# repo_url = \"https://huggingface.co/judynguyen16/graphcodebert--code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Lightning Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10300/10300 [00:22<00:00, 457.98it/s]\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 54 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "11/26/2024 17:49:52 - INFO - __main__ -   ***** Running training *****\n",
      "11/26/2024 17:49:52 - INFO - __main__ -     Num examples = 10300\n",
      "11/26/2024 17:49:52 - INFO - __main__ -     Batch size = 64\n",
      "11/26/2024 17:49:52 - INFO - __main__ -     Num epoch = 2\n"
     ]
    }
   ],
   "source": [
    "# # ---------*****---------\n",
    "# # Prepare training loader and fine-tuning\n",
    "\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "# config = config_class.from_pretrained(args.config_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "# train_examples = read_examples(args.train_filename)\n",
    "# train_features = convert_examples_to_features(train_examples, tokenizer, args,stage='train')\n",
    "# train_data = TextDataset(train_features,args)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=54)\n",
    "# num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "# #Start training\n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "# logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "# logger.info(\"  Num epoch = %d\", args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 399.43it/s]\n",
      "11/26/2024 21:00:58 - INFO - __main__ -   \n",
      "***** Running evaluation *****\n",
      "11/26/2024 21:00:58 - INFO - __main__ -     Num examples = 500\n",
      "11/26/2024 21:00:58 - INFO - __main__ -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# validation loader\n",
    "dev_dataset={}\n",
    "if 'dev_loss' in dev_dataset:\n",
    "    eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "else:\n",
    "    eval_examples = read_examples(args.dev_filename)\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
    "    eval_data = TextDataset(eval_features,args)\n",
    "    dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=54)\n",
    "\n",
    "logger.info(\"\\n***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2024 21:02:00 - INFO - __main__ -   Test file: data/valid.java-cs.txt.java,data/valid.java-cs.txt.cs\n",
      "100%|██████████| 50/50 [00:00<00:00, 561.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "files=[]\n",
    "if args.dev_filename is not None:\n",
    "    files.append(args.dev_filename)\n",
    "if args.test_filename is not None:\n",
    "    files.append(args.test_filename)\n",
    "    \n",
    "for idx,file in enumerate(files):   \n",
    "    logger.info(\"Test file: {}\".format(file))\n",
    "    eval_examples = read_examples(file)\n",
    "    eval_examples = eval_examples[:50]\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "    eval_data = TextDataset(eval_features,args) \n",
    "\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    test_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LightningModule model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload model from saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "\n",
    "#budild model\n",
    "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
    "                beam_size=args.beam_size,max_length=args.max_target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "# if args.load_model_path is not None:\n",
    "pretrained_model_path = \"saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\"\n",
    "print(\"reload model from {}\".format(pretrained_model_path))\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# np.bool = np.bool_\n",
    "# model.eval()\n",
    "# dev_bleu,xmatch=calculate_bleu(model, eval_examples, test_dataloader)\n",
    "# print(f\"BLEU: {dev_bleu}, X-Match: {xmatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:43<00:00, 43.94s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "p=[]\n",
    "for batch in tqdm(test_dataloader,total=len(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                    \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids,source_mask,position_idx,att_mask, None, None)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "accs = []\n",
    "with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "    for ref, gold in zip(p, eval_examples):\n",
    "        predictions.append(ref)\n",
    "        f.write(ref+'\\n')\n",
    "        f1.write(gold.target+'\\n')     \n",
    "        accs.append(ref==gold.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public DVRecord(RecordInputStream in1){_option_flags = in1.ReadInt();_promptTitle = in1.ReadInt();_errorTitle = in1.ReadUnicodeString(in1);_errorTitle = in1.ReadUnicodeString(in1);int field_1 = in1.ReadUShort();_size_1 = in1.ReadShort();_size_1 = in1.ReadShort();_not_used_formula = in1.ReadShort();_formula1 = NPOI.Read(field_size_first_formula, in1.ReadShort();_formula2 = Formula.Read(field_size_size_formula, in1);_regions = Formula.Read(in1);}',\n",
       " 'public override string ToString(){return pattern();}',\n",
       " 'public InsertInstanceRequest(): base(\"Ots\", \"2016-06-20\", \"InsertInstance\", \"ots\", \"openAPI\"){Method = MethodType.POST;}',\n",
       " 'public override bool contains(object o){return indexOf(o) != -1;}',\n",
       " 'public java.nio.ByteBuffer encode(string s){return encode(java.nio.CharBuffer.wrap(s));}',\n",
       " 'public override bool RequiresCommitBody(){return false;}',\n",
       " 'public virtual string GetKey(){return RawParseUtils.Decode(enc, buffer, keyStart, keyEnd);}',\n",
       " 'public override ValueEval Evaluate(int srcRowIndex, int srcColumnIndex, ValueEval arg0, ValueEval arg1,ValueEval arg2,ValueEval arg3,ValueEval arg4,ValueEval arg4){double result;try{double d0 = NumericFunction.SingleOperandEvaluate(arg0, srcRowIndex, srcColumnIndex);double d1 = NumericFunction.SingleOperandEvaluate(arg1, srcRowIndex, srcColumnIndex);double d2 = NumericFunction.SingleOperandEvaluate(arg2, srcRowIndex, srcColumnIndex);double d3 = NumericFunction.SingleOperandEvaluate(arg4, srcRowIndex, srcColumnIndex);double d4 = Evaluate(d0, d1, d2, d2, d3 != 0.0);NumericFunction.CheckValue(result);}catch (EvaluationException e){return e.GetErrorEval();}return new NumberEval(result);}',\n",
       " 'public virtual DeleteClientVpnEndpointResponse DeleteClientVpnEndpoint(DeleteClientVpnEndpointRequest request){var options = new InvokeOptions();options.RequestMarshaller = DeleteClientVpnEndpointRequestMarshaller.Instance;options.ResponseUnmarshaller = DeleteClientVpnEndpointResponseUnmarshaller.Instance;return Invoke<DeleteClientVpnEndpointResponse>(request, options);}',\n",
       " 'public virtual object Get(string key){IList<TernaryTreeNode> list = _automplete.PrefixCompletion(root, key, 0);if (list == null || list.IsEmpty()){return null;}foreach (TernaryTreeNode n in list){if (CharSeqEquals(n.token, key)){return n.val;}}return null;}',\n",
       " 'public virtual StartFleetActionsResponse StartFleetActions(StartFleetActionsRequest request){var options = new InvokeOptions();options.RequestMarshaller = StartFleetActionsRequestMarshaller.Instance;options.ResponseUnmarshaller = StartFleetActionsResponseUnmarshaller.Instance;return Invoke<StartFleetActionsResponse>(request, options);}',\n",
       " 'public CellRangeAddress GetCellRangeAddress(int index){return _list[index];}',\n",
       " 'public static XmlDocument LoadXML(XmlReader input){XmlDocument result = new XmlDocument();try{result.Load(input);}catch (Exception se){throw new Exception(\"Error parsing file:\" + se, se);}return result;}',\n",
       " \"public virtual double Get(string name, double dflt){double[] vals;object temp;if (valByRound.TryGetValue(name, out temp) && temp != null){vals = (double[])temp;return vals[roundNumber % vals.Length];}string sval;if (!props.TryGetValue(name, out sval)){sval = dflt.ToString(CultureInfo.InvariantCulture);}if (sval.IndexOf(':') < 0){return double.Parse(sval, CultureInfo.InvariantCulture);}int k = sval.IndexOf(':');string colName = sval.Substring(0, k - 0);sval = sval.Substring(k + 1);colForValByRound[name] = colName;vals = PropToDoubleArray(sval);valByRound[name] = vals;return vals[roundNumber % vals.Length];}\",\n",
       " 'public int GetBackgroundImageId(){EscherSimpleProperty property = (EscherSimpleProperty)GetOptRecord().Lookup(EscherProperties.FILL__PATTERNTEXTURE);return property == null ? 0 : property.PropertyValue;}',\n",
       " 'public virtual TreeFilter GetTreeFilter(){return treeFilter;}',\n",
       " 'public virtual GetMemberResponse GetMember(GetMemberRequest request){var options = new InvokeOptions();options.RequestMarshaller = GetMemberRequestMarshaller.Instance;options.ResponseUnmarshaller = GetMemberResponseUnmarshaller.Instance;return Invoke<GetMemberResponse>(request, options);}',\n",
       " 'public override bool CanEncode(){return true;}',\n",
       " 'public virtual ReplaceRouteResponse ReplaceRoute(ReplaceRouteRequest request){var options = new InvokeOptions();options.RequestMarshaller = ReplaceRouteRequestMarshaller.Instance;options.ResponseUnmarshaller = ReplaceRouteResponseUnmarshaller.Instance;return Invoke<ReplaceRouteResponse>(request, options);}',\n",
       " 'public virtual ObjectId GetResultTreeId(){return (resultTree == null) ? null : resultTree.ToObjectId();}',\n",
       " 'public override bool Equals(Object o){bool rval = this == o;if (!rval && ((o != null) && (o.GetType() == this.GetType())){rval = (IntList)o;if (other._limit == _limit){rval = true;for (int j = 0; rval && (j < _limit); j++){rval = _array[j] == other._array[j];}}}return rval;}',\n",
       " 'public virtual ListReusableDelegationSetsResponse ListReusableDelegationSets(ListReusableDelegationSetsRequest request){var options = new InvokeOptions();options.RequestMarshaller = ListReusableDelegationSetsRequestMarshaller.Instance;options.ResponseUnmarshaller = ListReusableDelegationSetsResponseUnmarshaller.Instance;return Invoke<ListReusableDelegationSetsResponse>(request, options);}',\n",
       " 'public override string ToString(){return \"(\" + a.ToString() + \" OR \" + b.ToString() + \")\";}',\n",
       " 'public virtual InitiateLayerUploadResponse InitiateLayerUpload(InitiateLayerUploadRequest request){var options = new InvokeOptions();options.RequestMarshaller = InitiateLayerUploadRequestMarshaller.Instance;options.ResponseUnmarshaller = InitiateLayerUploadResponseUnmarshaller.Instance;return Invoke<InitiateLayerUploadResponse>(request, options);}',\n",
       " 'public UpdateRepoRequest(): base(\"cr\", \"2016-06-07\", \"UpdateRepo\", \"cr\", \"openAPI\"){UriPattern = \"/repos/[RepoNamespace]/[RepoName]\";Method = MethodType.POST;}',\n",
       " 'public PhoneticFilterFactory(IDictionary<string, string> args): base(args){inject = GetBoolean(args, INJECT, true);name = Require(args, ENCODER);string v = Get(args, MAX_CODE_LENGTH);if (v != null){maxCodeLength = int.Parse(v);}else{maxCodeLength = null;}if (args.Count > 0){throw new System.ArgumentException(\"Unknown parameters: \" + args);}}',\n",
       " 'public virtual FetchCommand Fetch(){return new FetchCommand(repo);}',\n",
       " 'public virtual QueryPhraseMap SearchPhrase(string fieldName, IList<TermInfo> phraseCandidate){QueryPhraseMap root = GetRootMap(fieldName);if (root == null){return null;}return root.SearchPhrase(phraseCandidate);}',\n",
       " 'public override java.util.Iterator<java.util.MapClass.Entry<K, V>> iterator(){return new java.util.Hashtable<K, V>.EntryIterator(this._enclosing);}',\n",
       " 'public virtual DeleteDBSnapshotResponse DeleteDBSnapshot(DeleteDBSnapshotRequest request){var options = new InvokeOptions();options.RequestMarshaller = DeleteDBSnapshotRequestMarshaller.Instance;options.ResponseUnmarshaller = DeleteDBSnapshotResponseUnmarshaller.Instance;return Invoke<DeleteDBSnapshotResponse>(request, options);}',\n",
       " 'public virtual void SetOutput(){output = true;}',\n",
       " 'public override java.nio.ByteBuffer compact(){throw new System.NotImplementedException();}',\n",
       " 'public virtual org.v1.PullParser NewPullParser(){if (parserClasses == null){throw new org.v1.v1.v1.v1;}if (parserClasses.size() == 0){throw new org.v1.v1.v1.v1.Size(\"No valid parser classes found in \"+ classNamesLocation);}java.lang.StringBuilder issues = new java.lang.StringBuilder();for (int i = 0; i < parserClasses.size(); i++){var ppClass = (System.Type)parserClass = (string)parserClasses[i];try{org.v1 = (string)parserClasses.get();{for (int i = 0; i < parser.key; i++){var key = (string)iterator.getValue(key, true);if (value != null && value != null && value.TryGetValue(key, true);)){pp.setFeature(key, true);}}return pp;}catch (Exception e){issues.append(\"could not create parser: \" + ex.ToString() + \" + ex.ToString() + \"; \"; \" + ex.',\n",
       " 'public virtual DeleteAnalysisSchemeResponse DeleteAnalysisScheme(DeleteAnalysisSchemeRequest request){var options = new InvokeOptions();options.RequestMarshaller = DeleteAnalysisSchemeRequestMarshaller.Instance;options.ResponseUnmarshaller = DeleteAnalysisSchemeResponseUnmarshaller.Instance;return Invoke<DeleteAnalysisSchemeResponse>(request, options);}',\n",
       " 'public ExcelExtractor(HSSFWorkbook wb): base(wb){_wb = wb;_formatter = new HSSFDataFormatter();}',\n",
       " 'public override java.nio.IntBuffer put(int index, int c){checkIndex(index);byteBuffer.putInt(index * libcore.io.SizeOf.INT, c);return this;}',\n",
       " 'public byte GetParameterClass(int index){if (index >= paramClass.Length){return paramClass[paramClass.Length - 1];}return paramClass[index];}',\n",
       " 'public virtual ListEndpointsResponse ListEndpoints(ListEndpointsRequest request){var options = new InvokeOptions();options.RequestMarshaller = ListEndpointsRequestMarshaller.Instance;options.ResponseUnmarshaller = ListEndpointsResponseUnmarshaller.Instance;return Invoke<ListEndpointsResponse>(request, options);}',\n",
       " 'public static CharsRef Join(string[] words, CharsRef reuse){int upto = 0;char[] buffer = reuse.Chars;foreach (string word in words){int wordLen = word.Length;int need = (0 == upto ? wordLen : 1 + upto + wordLen);if (needle > buffer.Length){reuse.Grow(needle ? wordLen : 1 + upto + wordLen);}if (upto > 0){buffer[upto++] = SynonymMap.WORD_SEPARER;}word.CopyTo(0, wordLen, buffer, upto);upto += wordLen;}reuse.Length = upto;return reuse.Get();}',\n",
       " 'public java.lang.StringBuffer insert(int index, float f){return insert(index, System.Convert.ToString(f));}',\n",
       " 'public override java.nio.ShortBuffer put(short[] src, int srcOffset, int shortCount){if (shortCount > remaining()){throw new java.nio.BufferOverflowException();}System.Array.Copy(src, srcOffset, backingArray, offset + _position, shortCount);_position += shortCount;return this;}',\n",
       " 'public virtual DisassociateResolverEndpointIpAddressResponse DisassociateResolverEndpointIpAddress(DisassociateResolverEndpointIpAddressRequest request){var options = new InvokeOptions();options.RequestMarshaller = DisassociateResolverEndpointIpAddressRequestMarshaller.Instance;options.ResponseUnmarshaller = DisassociateResolverEndpointIpAddressResponseUnmarshaller.Instance;return Invoke<DisassociateResolverEndpointIpAddressResponse>(request, options);}',\n",
       " 'public virtual AcceptDirectConnectGatewayAssociationProposalResponse AcceptDirectConnectGatewayAssociationProposal(AcceptDirectConnectGatewayAssociationProposalRequest request){var options = new InvokeOptions();options.RequestMarshaller = AcceptDirectConnectGatewayAssociationProposalRequestMarshaller.Instance;options.ResponseUnmarshaller = AcceptDirectConnectGatewayAssociationProposalResponseUnmarshaller.Instance;return Invoke<AcceptDirectConnectGatewayAssociationProposalResponse>(request, options);}',\n",
       " 'public virtual StopStackSetOperationResponse StopStackSetOperation(StopStackSetOperationRequest request){var options = new InvokeOptions();options.RequestMarshaller = StopStackSetOperationRequestMarshaller.Instance;options.ResponseUnmarshaller = StopStackSetOperationResponseUnmarshaller.Instance;return Invoke<StopStackSetOperationResponse>(request, options);}',\n",
       " 'public virtual CreateCacheSubnetGroupResponse CreateCacheSubnetGroup(CreateCacheSubnetGroupRequest request){var options = new InvokeOptions();options.RequestMarshaller = CreateCacheSubnetGroupRequestMarshaller.Instance;options.ResponseUnmarshaller = CreateCacheSubnetGroupResponseUnmarshaller.Instance;return Invoke<CreateCacheSubnetGroupResponse>(request, options);}',\n",
       " 'public CachedOrds(OrdinalsSegmentReader source, int maxDoc){offsets = new int[maxDoc + 1];int[] ords = new int[maxDoc];long totalOrds = 0;Int32sRef values = new Int32sRef(32);for (int docID = 0; docID < maxDoc; docID++){offsets[docID] = (int)totalOrds;source.Get(docID, values);long nextLength = totalOrds + values.Length;if (nextLength > ords.Length){throw new InvalidOperationException(\"too many ordinals (>= \" + nextLength + \") to cache\");}ords = ArrayUtil.Grow(ords, 0, ords, (int)nextLength);}',\n",
       " 'public virtual string GetRawUserInfo(){return userInfo;}',\n",
       " 'public override object[] toArray(){return objectArrays.toArrayImpl(this);}',\n",
       " 'public virtual DescribeCompilationJobResponse DescribeCompilationJob(DescribeCompilationJobRequest request){var options = new InvokeOptions();options.RequestMarshaller = DescribeCompilationJobRequestMarshaller.Instance;options.ResponseUnmarshaller = DescribeCompilationJobResponseUnmarshaller.Instance;return Invoke<DescribeCompilationJobResponse>(request, options);}',\n",
       " 'public string getQuery(){return decode(query);}']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "xmatch=round(np.mean(accs)*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 74.33, X-Match: 58.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"BLEU: {dev_bleu}, X-Match: {xmatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public override string ToString(){return pattern();}\n"
     ]
    }
   ],
   "source": [
    "print(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login huggerface\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mgit version 2.25.1\u001b[0m\n",
      "\u001b[1m\u001b[31mLooks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).\u001b[0m\n",
      "\n",
      "You are about to create \u001b[1mjudynguyen16/judynguyen16/graphcodebert--code-translation-java-cs\u001b[0m\n",
      "Proceed? [Y/n] ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 285, in run\n",
      "    choice = input(\"Proceed? [Y/n] \").lower()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# repo_id = \"judynguyen16/graphcodebert-code-translation-java-cs\"\n",
    "# !huggingface-cli repo create \"judynguyen16/graphcodebert-code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Cannot have -- or .. in repo_id: 'judynguyen16/graphcodebert--code-translation-java-cs'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m files_to_upload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/judy/transformer-code-translation/GraphCodeBERT/translation/saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dest_path, local_path \u001b[38;5;129;01min\u001b[39;00m files_to_upload\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:167\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have -- or .. in repo_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.git\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo_id cannot end by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.git\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Cannot have -- or .. in repo_id: 'judynguyen16/graphcodebert--code-translation-java-cs'."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "# Push model and tokenizer to Hugging Face Hub\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "repo_id = \"judynguyen16/graphcodebert--code-translation-java-cs\"\n",
    "\n",
    "files_to_upload = {\n",
    "    \"pytorch_model.bin\": \"/home/ubuntu/judy/transformer-code-translation/GraphCodeBERT/translation/saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\",\n",
    "}\n",
    "\n",
    "for dest_path, local_path in files_to_upload.items():\n",
    "    upload_file(\n",
    "        path_or_fileobj=local_path,\n",
    "        path_in_repo=dest_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
