{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"java\"\n",
    "target=\"cs\"\n",
    "lr=1e-4\n",
    "batch_size=8\n",
    "beam_size=10\n",
    "source_length=320\n",
    "target_length=256\n",
    "output_dir=f\"saved_models/{source}-{target}/\"\n",
    "train_file=f\"data/train.java-cs.txt.{source},data/train.java-cs.txt.{target}\"\n",
    "dev_file=f\"data/valid.java-cs.txt.{source},data/valid.java-cs.txt.{target}\"\n",
    "epochs=100\n",
    "pretrained_model=\"microsoft/graphcodebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/microsoft/CodeBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install tree_sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd GraphCodeBERT/translation\n",
    "# !cd parser\n",
    "# !bash build.sh\n",
    "# !cd ..\n",
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/judy/miniconda3/envs/fisc/lib/python3.12/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'ruby':DFG_ruby,\n",
    "    'go':DFG_go,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#load parsers\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    # print(Language)\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 source,\n",
    "                 target,\n",
    "                 lang\n",
    "                 ):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lang=lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass    \n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"    \n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "        root_node = tree.root_node  \n",
    "        tokens_index=tree_to_token_index(root_node)     \n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]  \n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)  \n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{}) \n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    source,target=filename.split(',')\n",
    "    lang='java'\n",
    "    if source[-1]=='s':\n",
    "        lang='c_sharp'\n",
    "        \n",
    "    with open(source,encoding=\"utf-8\") as f1,open(target,encoding=\"utf-8\") as f2:\n",
    "        for line1,line2 in zip(f1,f2):\n",
    "            line1=line1.strip()\n",
    "            line2=line2.strip()\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    source=line1,\n",
    "                    target=line2,\n",
    "                    lang=lang\n",
    "                        ) \n",
    "            )\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the input for BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,                 \n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg=extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))    \n",
    "        code_tokens=[y for x in code_tokens for y in x]  \n",
    "        \n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:args.max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        dfg=dfg[:args.max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=args.max_source_length-len(source_ids)\n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length      \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length        \n",
    "        \n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)    \n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]        \n",
    "      \n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                logger.info(\"position_idx: {}\".format(position_idx))\n",
    "                logger.info(\"dfg_to_code: {}\".format(' '.join(map(str, dfg_to_code))))\n",
    "                logger.info(\"dfg_to_dfg: {}\".format(' '.join(map(str, dfg_to_dfg))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: <__main__.Args object at 0x7d6424a3ea20>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Simulate argparse for Jupyter\n",
    "class Args:\n",
    "    model_type = \"roberta\"\n",
    "    model_name_or_path = \"roberta-base\"\n",
    "    output_dir = \"./output\"\n",
    "    load_model_path = None\n",
    "    train_filename = None\n",
    "    dev_filename = None\n",
    "    test_filename = None\n",
    "    source_lang = \"en\"\n",
    "    config_name = \"\"\n",
    "    tokenizer_name = \"\"\n",
    "    max_source_length = 64\n",
    "    max_target_length = 32\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_test = False\n",
    "    do_lower_case = False\n",
    "    no_cuda = False\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 256\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 5e-5\n",
    "    beam_size = 10\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps = -1\n",
    "    eval_steps = -1\n",
    "    train_steps = -1\n",
    "    warmup_steps = 0\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "\n",
    "# Create an instance of Args\n",
    "args = Args()\n",
    "\n",
    "# Print the arguments for verification\n",
    "print(f\"Arguments: {args}\")\n",
    "args.model_type = \"roberta\"\n",
    "args.source_lang = source\n",
    "args.target_lang = target\n",
    "args.model_name_or_path = pretrained_model\n",
    "args.tokenizer_name = \"microsoft/graphcodebert-base\"\n",
    "args.config_name = \"microsoft/graphcodebert-base\"\n",
    "args.train_filename = train_file\n",
    "args.dev_filename = dev_file\n",
    "args.output_dir = output_dir\n",
    "args.learning_rate = lr\n",
    "args.num_train_epochs = epochs\n",
    "args.train_batch_size = batch_size\n",
    "args.eval_batch_size = batch_size\n",
    "args.max_source_length = source_length\n",
    "args.max_target_length = target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parsers for each language\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples, args):\n",
    "        self.examples = examples\n",
    "        self.args=args  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #calculate graph-guided masked function\n",
    "        attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)\n",
    "        #calculate begin index of node and max length of input\n",
    "        node_index=sum([i>1 for i in self.examples[item].position_idx])\n",
    "        max_length=sum([i!=1 for i in self.examples[item].position_idx])\n",
    "        #sequence can attend to sequence\n",
    "        attn_mask[:node_index,:node_index]=True\n",
    "        #special tokens attend to all tokens\n",
    "        for idx,i in enumerate(self.examples[item].source_ids):\n",
    "            if i in [0,2]:\n",
    "                attn_mask[idx,:max_length]=True\n",
    "        #nodes attend to code tokens that are identified from\n",
    "        for idx,(a,b) in enumerate(self.examples[item].dfg_to_code):\n",
    "            if a<node_index and b<node_index:\n",
    "                attn_mask[idx+node_index,a:b]=True\n",
    "                attn_mask[a:b,idx+node_index]=True\n",
    "        #nodes attend to adjacent nodes         \n",
    "        for idx,nodes in enumerate(self.examples[item].dfg_to_dfg):\n",
    "            for a in nodes:\n",
    "                if a+node_index<len(self.examples[item].position_idx):\n",
    "                    attn_mask[idx+node_index,a+node_index]=True  \n",
    "                    \n",
    "        return (torch.tensor(self.examples[item].source_ids),\n",
    "                torch.tensor(self.examples[item].source_mask),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask), \n",
    "                torch.tensor(self.examples[item].target_ids),\n",
    "                torch.tensor(self.examples[item].target_mask),)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA, GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dir if output_dir not exist\n",
    "if os.path.exists(args.output_dir) is False:\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: RobertaTokenizer(name_or_path='microsoft/graphcodebert-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name )\n",
    "print(f\"tokenizer: {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
    "                beam_size=args.beam_size,max_length=args.max_target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "if args.load_model_path is not None:\n",
    "    logger.info(\"reload model from {}\".format(args.load_model_path))\n",
    "    model.load_state_dict(torch.load(args.load_model_path))\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap this code to pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class Seq2SeqLightning(pl.LightningModule):\n",
    "    def __init__(self, model_class, model_name_or_path, config, tokenizer, args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = model_class.from_pretrained(model_name_or_path, config=config)\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.hidden_size, \n",
    "            nhead=config.num_attention_heads\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        \n",
    "        # Seq2Seq Model\n",
    "        self.model = Seq2Seq(\n",
    "            encoder=self.encoder,\n",
    "            decoder=self.decoder,\n",
    "            config=config,\n",
    "            beam_size=args.beam_size,\n",
    "            max_length=args.max_target_length,\n",
    "            sos_id=tokenizer.cls_token_id,\n",
    "            eos_id=tokenizer.sep_token_id\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained weights if specified\n",
    "        if args.load_model_path is not None:\n",
    "            self.log(f\"Reloading model from {args.load_model_path}\")\n",
    "            state_dict = torch.load(args.load_model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        # self.args = args                                                                                                  \n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # loss = self.model(**batch)  # Assuming the model returns the loss\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "        loss, _, _ = model(source_ids,source_mask,position_idx,att_mask,target_ids,target_mask)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self.model(**batch)  # Assuming the model returns the loss\n",
    "    #     self.log(\"val_loss\", loss)\n",
    "    #     return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Initialize evaluation results and flags\n",
    "        eval_loss, tokens_num = 0, 0\n",
    "        eval_ppl, dev_bleu, xmatch = None, None, None\n",
    "\n",
    "        # Unpack the batch\n",
    "        source_ids, source_mask, position_idx, att_mask, target_ids, target_mask = batch\n",
    "\n",
    "        # Forward pass for loss calculation\n",
    "        with torch.no_grad():\n",
    "            _, loss, num = self.model(\n",
    "                source_ids=source_ids,\n",
    "                source_mask=source_mask,\n",
    "                position_idx=position_idx,\n",
    "                att_mask=att_mask,\n",
    "                target_ids=target_ids,\n",
    "                target_mask=target_mask,\n",
    "            )\n",
    "\n",
    "        # Accumulate evaluation metrics\n",
    "        eval_loss += loss.sum().item()\n",
    "        tokens_num += num.sum().item()\n",
    "\n",
    "        # Compute perplexity\n",
    "        eval_loss = eval_loss / tokens_num\n",
    "        eval_ppl = round(np.exp(eval_loss), 5)\n",
    "\n",
    "        # Log validation loss and perplexity\n",
    "        self.log(\"val_loss\", eval_loss, prog_bar=True, logger=True)\n",
    "        self.log(\"val_ppl\", eval_ppl, prog_bar=True, logger=True)\n",
    "\n",
    "        # BLEU and xMatch evaluation\n",
    "        predictions = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(source_ids, source_mask, position_idx, att_mask)\n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy().tolist()\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "                predictions.append(text)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        for pred, gold in zip(predictions, batch[\"target_text\"]):  # Assuming ground truth is in batch[\"target_text\"]\n",
    "            accs.append(pred == gold)\n",
    "\n",
    "        # Save predictions and ground truth for BLEU calculation\n",
    "        dev_bleu = round(\n",
    "            _bleu(predictions, batch[\"target_text\"]), 2\n",
    "        )  # Replace `_bleu` with your BLEU scoring implementation\n",
    "        xmatch = round(np.mean(accs) * 100, 4)\n",
    "\n",
    "        # Log BLEU and xMatch scores\n",
    "        self.log(\"val_bleu\", dev_bleu, prog_bar=True, logger=True)\n",
    "        self.log(\"val_xmatch\", xmatch, prog_bar=True, logger=True)\n",
    "\n",
    "        # Save best model checkpoint based on BLEU+xMatch\n",
    "        metric = dev_bleu + xmatch\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.save_checkpoint(\"best_bleu\", self.model)\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": eval_loss,\n",
    "            \"val_ppl\": eval_ppl,\n",
    "            \"val_bleu\": dev_bleu,\n",
    "            \"val_xmatch\": xmatch,\n",
    "        }\n",
    "\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     loss = self.model(**batch)  # Assuming the model returns the loss\n",
    "    #     self.log(\"test_loss\", loss)\n",
    "    #     return loss\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        p = []  # Predictions list\n",
    "        source_ids, source_mask, position_idx, att_mask, target_ids, target_mask = batch\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(source_ids, source_mask, position_idx, att_mask)\n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy().tolist()\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "\n",
    "        # Compute BLEU and Exact Match (xMatch)\n",
    "        predictions, accs = [], []\n",
    "        for ref, gold in zip(p, batch[\"target_text\"]):  # Assuming ground truth is in batch[\"target_text\"]\n",
    "            predictions.append(ref)\n",
    "            accs.append(ref == gold)\n",
    "\n",
    "        # Save predictions and ground truth\n",
    "        output_file = os.path.join(self.hparams.args.output_dir, f\"test_{batch_idx}.output\")\n",
    "        gold_file = os.path.join(self.hparams.args.output_dir, f\"test_{batch_idx}.gold\")\n",
    "\n",
    "        with open(output_file, \"w\") as f_pred, open(gold_file, \"w\") as f_gold:\n",
    "            for pred, gold in zip(predictions, batch[\"target_text\"]):\n",
    "                f_pred.write(pred + \"\\n\")\n",
    "                f_gold.write(gold + \"\\n\")\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        dev_bleu = round(\n",
    "            _bleu(gold_file, output_file), 2\n",
    "        )  # Replace `_bleu` with your BLEU calculation logic\n",
    "\n",
    "        # Log metrics\n",
    "        xmatch = round(np.mean(accs) * 100, 4)\n",
    "        self.log(\"test_bleu\", dev_bleu, prog_bar=True, logger=True)\n",
    "        self.log(\"test_xmatch\", xmatch, prog_bar=True, logger=True)\n",
    "\n",
    "        logger.info(\"  %s = %s \" % (\"bleu-4\", str(dev_bleu)))\n",
    "        logger.info(\"  %s = %s \" % (\"xMatch\", str(xmatch)))\n",
    "        logger.info(\"  \" + \"*\" * 20)\n",
    "\n",
    "        return {\"test_bleu\": dev_bleu, \"test_xmatch\": xmatch}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, *args, **kwargs):\n",
    "        # optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.args.learning_rate)\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': args.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_scheduler(self):\n",
    "        return get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=self.total_steps\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.n_gpu > 1:\n",
    "    # multi-gpu training\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------- TRAINING -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dev_dataset):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0                     \n",
    "    eval_flag=False    \n",
    "    if 'dev_loss' in dev_dataset:\n",
    "        eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "    else:\n",
    "        eval_examples = read_examples(args.dev_filename)\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args, stage='dev')\n",
    "        eval_data = TextDataset(eval_features,args)\n",
    "        dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "\n",
    "    logger.info(\"\\n***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    #Start Evaling model\n",
    "    model.eval()\n",
    "    eval_loss,tokens_num = 0,0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)               \n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "        with torch.no_grad():\n",
    "            _,loss,num = model(source_ids,source_mask,position_idx,att_mask,target_ids,target_mask)     \n",
    "        eval_loss += loss.sum().item()\n",
    "        tokens_num += num.sum().item()\n",
    "    \n",
    "    eval_loss = eval_loss / tokens_num\n",
    "    model.train()\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir, step):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model \n",
    "    output_model_file = os.path.join(output_dir, \"model.{}.bin\".format(step))\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10300 [00:00<?, ?it/s]11/17/2024 18:14:41 - INFO - __main__ -   *** Example ***\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Result', '_list', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_', '_(', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_ids: 0 15110 9527 29235 7529 35615 3999 35571 565 40981 48136 889 29235 7529 35615 3999 35571 565 40981 1437 36 9527 29235 7529 35615 3999 35571 565 40981 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 36583 29235 7529 35615 3999 35571 565 40981 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_code: (30, 31) (33, 34) (33, 34) (35, 39) (40, 41) (54, 55)\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '(', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_ids: 0 15110 6229 9527 29235 7529 35615 3999 35571 565 40981 47806 9527 29235 7529 35615 3999 35571 565 40981 1640 36583 29235 7529 35615 3999 35571 565 40981 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 36583 29235 7529 35615 3999 35571 565 40981 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   *** Example ***\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Update', 'J', 'ourney', 'State', 'Result', '_update', 'J', 'ourney', 'State', '_', '_(', '_Update', 'J', 'ourney', 'State', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'Update', 'J', 'ourney', 'State', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_ids: 0 15110 14686 863 37786 13360 48136 2935 863 37786 13360 1437 36 14686 863 37786 13360 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 39962 863 37786 13360 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_code: (18, 19) (21, 22) (21, 22) (23, 27) (28, 29) (38, 39)\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_Update', 'J', 'ourney', 'State', 'Response', '_Update', 'J', 'ourney', 'State', '(', 'Update', 'J', 'ourney', 'State', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'Update', 'J', 'ourney', 'State', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_ids: 0 15110 6229 14686 863 37786 13360 47806 14686 863 37786 13360 1640 39962 863 37786 13360 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 14686 863 37786 13360 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 14686 863 37786 13360 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 39962 863 37786 13360 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   *** Example ***\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_remove', 'Present', 'ation', 'Format', '_', '_(', '_)', '_{', '_remove', '1', 'st', 'Property', '_(', '_Property', 'ID', 'Map', '_.', '_PID', '_', 'PRES', 'FORM', 'AT', '_)', '_;', '_}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_ids: 0 15110 13842 3438 45195 1258 48587 1437 36 4839 25522 3438 134 620 44720 36 10491 2688 41151 479 47102 1215 29679 38036 2571 4839 25606 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_code: \n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_dfg: \n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_void', '_Remove', 'Present', 'ation', 'Format', '(){', 'M', 'utable', 'Section', '_s', '_=', '_(', 'M', 'utable', 'Section', ')', 'First', 'Section', ';', 's', '.', 'Remove', 'Property', '(', 'Property', 'ID', 'Map', '.', 'P', 'ID', '_', 'PRES', 'FORM', 'AT', ');', '}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_ids: 0 15110 13842 27336 45195 1258 48587 49215 448 41280 43480 579 5457 36 448 41280 43480 43 10993 43480 131 29 4 47583 44720 1640 44720 2688 41151 4 510 2688 1215 29679 38036 2571 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   *** Example ***\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '_(', '_int', '_first', 'Row', '_,', '_int', '_last', 'Row', '_,', '_int', '_first', 'Col', '_,', '_int', '_last', 'Col', '_)', '_{', '_this', '_(', '_)', '_;', '_add', 'Cell', 'Range', 'Address', '_(', '_first', 'Row', '_,', '_first', 'Col', '_,', '_last', 'Row', '_,', '_last', 'Col', '_)', '_;', '_}', '</s>', 'firstRow', 'lastRow', 'firstCol', 'lastCol', 'firstRow', 'firstCol', 'lastRow', 'lastCol']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_ids: 0 15110 13696 43430 46486 36583 36 6979 78 43277 2156 6979 94 43277 2156 6979 78 18551 2156 6979 94 18551 4839 25522 42 36 4839 25606 1606 40216 43430 46486 36 78 43277 2156 78 18551 2156 94 43277 2156 94 18551 4839 25606 35524 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_code: (8, 10) (12, 14) (16, 18) (20, 22) (33, 35) (36, 38) (39, 41) (42, 44)\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_dfg: [] [] [] [] [0] [2] [1] [3]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '(', 'int', '_first', 'Row', ',', '_int', '_last', 'Row', ',', '_int', '_first', 'Col', ',', '_int', '_last', 'Col', '):', '_this', '(){', 'Add', 'Cell', 'Range', 'Address', '(', 'first', 'Row', ',', '_first', 'Col', ',', '_last', 'Row', ',', '_last', 'Col', ');', '}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_ids: 0 15110 13696 43430 46486 36583 1640 2544 78 43277 6 6979 94 43277 6 6979 78 18551 6 6979 94 18551 3256 42 49215 20763 40216 43430 46486 1640 9502 43277 6 78 18551 6 94 43277 6 94 18551 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   *** Example ***\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_delete', '_', '_(', '_int', '_key', '_)', '_{', '_int', '_i', '_=', '_binary', 'Search', '_(', '_m', 'Keys', '_,', '_0', '_,', '_m', 'Size', '_,', '_key', '_)', '_;', '_if', '_(', '_i', '_>=', '_0', '_)', '_{', '_if', '_(', '_m', 'Values', '_[', '_i', '_]', '_!=', '_DE', 'LET', 'ED', '_)', '_{', '_m', 'Values', '_[', '_i', '_]', '_=', '_DE', 'LET', 'ED', '_;', '_m', 'Gar', 'bage', '_=', '_true', '_;', '_}', '_}', '_}', '</s>', 'key', 'i', 'i', 'i', 'i', 'i', 'binarySearch', 'mKeys', '0', 'mSize', 'key', 'i', 'i', 'DELETED', 'mValues', 'i', 'DELETED']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_ids: 0 15110 13842 17462 1437 36 6979 762 4839 25522 6979 939 5457 32771 39954 36 475 44534 2156 321 2156 475 45698 2156 762 4839 25606 114 36 939 49095 321 4839 25522 114 36 475 48738 646 939 27779 49333 5885 24258 1691 4839 25522 475 48738 646 939 27779 5457 5885 24258 1691 25606 475 31626 36772 5457 1528 25606 35524 35524 35524 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_code: (7, 8) (11, 12) (11, 12) (11, 12) (11, 12) (11, 12) (13, 15) (16, 18) (19, 20) (21, 23) (24, 25) (29, 30) (39, 40) (42, 45) (47, 49) (50, 51) (53, 56)\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   dfg_to_dfg: [] [6] [7] [8] [9] [10] [] [] [] [] [0] [5] [5] [] [16] [16] [13]\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_void', '_delete', '(', 'int', '_key', '){', 'int', '_i', '_=', '_binary', 'Search', '(', 'm', 'Keys', ',', '_0', ',', '_m', 'Size', ',', '_key', ');', 'if', '_(', 'i', '_>=', '_0', '){', 'if', '_(', 'm', 'Values', '[', 'i', ']', '_!=', '_DE', 'LET', 'ED', '){', 'm', 'Values', '[', 'i', ']', '_=', '_DE', 'LET', 'ED', ';', 'm', 'Gar', 'bage', '_=', '_true', ';', '}}}', '</s>']\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_ids: 0 15110 6229 13842 17462 1640 2544 762 48512 2544 939 5457 32771 39954 1640 119 44534 6 321 6 475 45698 6 762 4397 1594 36 118 49095 321 48512 1594 36 119 48738 10975 118 742 49333 5885 24258 1691 48512 119 48738 10975 118 742 5457 5885 24258 1691 131 119 31626 36772 5457 1528 131 49908 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/17/2024 18:14:41 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "100%|██████████| 10300/10300 [00:19<00:00, 517.79it/s]\n",
      "/home/judy/miniconda3/envs/fisc/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "11/17/2024 18:15:01 - INFO - __main__ -   ***** Running training *****\n",
      "11/17/2024 18:15:01 - INFO - __main__ -     Num examples = 10300\n",
      "11/17/2024 18:15:01 - INFO - __main__ -     Batch size = 8\n",
      "11/17/2024 18:15:01 - INFO - __main__ -     Num epoch = 100\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "model.train()\n",
    "train_examples = read_examples(args.train_filename)\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train')\n",
    "train_data = TextDataset(train_features,args)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=54)\n",
    "num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataloader)*args.num_train_epochs*0.1,num_training_steps=len(train_dataloader)*args.num_train_epochs)\n",
    "\n",
    "#Start training\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "logger.info(\"  Num epoch = %d\", args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(model, dev_dataset):\n",
    "    if 'dev_bleu' in dev_dataset:\n",
    "        eval_examples,eval_data=dev_dataset['dev_bleu']\n",
    "    else:\n",
    "        eval_examples = read_examples(args.dev_filename)\n",
    "        eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "        eval_data = TextDataset(eval_features,args)\n",
    "        dev_dataset['dev_bleu']=eval_examples,eval_data\n",
    "        \n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "    model.eval() \n",
    "    p=[]\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                 \n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "            for pred in preds:\n",
    "                t=pred[0].cpu().numpy()\n",
    "                t=list(t)\n",
    "                if 0 in t:\n",
    "                    t=t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    model.train()\n",
    "    predictions=[]\n",
    "    accs = []\n",
    "    with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
    "        for ref,gold in zip(p,eval_examples):\n",
    "            predictions.append(ref)\n",
    "            f.write(ref+'\\n')\n",
    "            f1.write(gold.target+'\\n')     \n",
    "            accs.append(ref==gold.target)\n",
    "\n",
    "    dev_bleu=round(_bleu(os.path.join(args.output_dir, \"dev.gold\"), os.path.join(args.output_dir, \"dev.output\")),2)\n",
    "    xmatch=round(np.mean(accs)*100,4)\n",
    "    return dev_bleu,xmatch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    files=[]\n",
    "    if args.dev_filename is not None:\n",
    "        files.append(args.dev_filename)\n",
    "    # if args.test_filename is not None:\n",
    "    #     files.append(args.test_filename)\n",
    "    for idx,file in enumerate(files):   \n",
    "        logger.info(\"Test file: {}\".format(file))\n",
    "        eval_examples = read_examples(file)\n",
    "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "        eval_data = TextDataset(eval_features,args) \n",
    "\n",
    "        # Calculate bleu\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "\n",
    "        model.eval() \n",
    "        p=[]\n",
    "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch                    \n",
    "            with torch.no_grad():\n",
    "                preds = model(source_ids,source_mask,position_idx,att_mask)  \n",
    "                for pred in preds:\n",
    "                    t=pred[0].cpu().numpy()\n",
    "                    t=list(t)\n",
    "                    if 0 in t:\n",
    "                        t=t[:t.index(0)]\n",
    "                    text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                    p.append(text)\n",
    "        model.train()\n",
    "        predictions=[]\n",
    "        accs=[]\n",
    "        with open(os.path.join(args.output_dir,\"test_{}.output\".format(str(idx))),'w') as f, open(os.path.join(args.output_dir,\"test_{}.gold\".format(str(idx))),'w') as f1:\n",
    "            for ref,gold in zip(p,eval_examples):\n",
    "                predictions.append(ref)\n",
    "                f.write(ref+'\\n')\n",
    "                f1.write(gold.target+'\\n')    \n",
    "                accs.append(ref==gold.target)\n",
    "        dev_bleu=round(_bleu(os.path.join(args.output_dir, \"test_{}.gold\".format(str(idx))).format(file), \n",
    "                                os.path.join(args.output_dir, \"test_{}.output\".format(str(idx))).format(file)),2)\n",
    "        logger.info(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
    "        logger.info(\"  %s = %s \"%(\"xMatch\",str(round(np.mean(accs)*100,4))))\n",
    "        logger.info(\"  \"+\"*\"*20)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# dev_dataset={}\n",
    "# nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
    "\n",
    "# for epoch in range(args.num_train_epochs):\n",
    "#     bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "#     for batch in bar:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "#         loss,_,_ = model(source_ids,source_mask,position_idx,att_mask,target_ids,target_mask)\n",
    "\n",
    "#         if args.n_gpu > 1:\n",
    "#             loss = loss.mean() # mean() to average on multi-gpu.\n",
    "#         if args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / args.gradient_accumulation_steps\n",
    "            \n",
    "#         tr_loss += loss.item()\n",
    "#         train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
    "#         bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
    "#         nb_tr_examples += source_ids.size(0)\n",
    "#         nb_tr_steps += 1\n",
    "#         loss.backward()\n",
    "\n",
    "#         if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
    "#             #Update parameters\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             scheduler.step()\n",
    "#             global_step += 1\n",
    "    \n",
    "#     # do eval \n",
    "#     if epoch in [ int(args.num_train_epochs*(i+1)//20) for i in range(20)]:\n",
    "#         eval_loss = eval(model, dev_dataset)\n",
    "#         result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
    "#                     'global_step': global_step+1,\n",
    "#                     'train_loss': round(train_loss,5)}\n",
    "#         for key in sorted(result.keys()):\n",
    "#             logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#         logger.info(\"  \"+\"*\"*20)   \n",
    "        \n",
    "#         # save last checkpoint\n",
    "#         last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "#         if not os.path.exists(last_output_dir):\n",
    "#             os.makedirs(last_output_dir)\n",
    "#         model_to_save = model.module if hasattr(model, 'module') else model\n",
    "#         output_model_file = os.path.join(last_output_dir, \"model.{}.bin\".format(global_step))\n",
    "#         torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#         if eval_loss<best_loss:\n",
    "#             best_loss=eval_loss\n",
    "#             best_output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
    "#             save_model(model, best_output_dir, global_step)\n",
    "#             logger.info(\"  Best checkpoint is saved in %s\",best_output_dir)\n",
    "#             logger.info(\"  \"+\"*\"*20)\n",
    "        \n",
    "#         # calculate bleu\n",
    "#         dev_bleu, xmatch = calculate_bleu(model, dev_dataset)\n",
    "#         logger.info(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
    "#         logger.info(\"  %s = %s \"%(\"xMatch\",str(xmatch)))\n",
    "#         logger.info(\"  \"+\"*\"*20)    \n",
    "#         if dev_bleu+xmatch>best_bleu:\n",
    "#             logger.info(\"  Best BLEU+xMatch:%s\",dev_bleu+xmatch)\n",
    "#             logger.info(\"  \"+\"*\"*20)\n",
    "#             best_bleu=dev_bleu+xmatch\n",
    "#             # Save best checkpoint for best bleu\n",
    "#             output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
    "#             if not os.path.exists(output_dir):\n",
    "#                 os.makedirs(output_dir)\n",
    "#             model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "#             output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "#             torch.save(model_to_save.state_dict(), output_model_file)\n",
    "# test(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_53684/3119324797.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and push to huggingface\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "path = \"/home/judy/code/code-transformer/CodeBERT/GraphCodeBERT/translation/saved_models/java-cs/checkpoint-best-ppl/pytorch_model.bin\"\n",
    "config = RobertaConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = Seq2Seq(encoder=RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\",config=config),\n",
    "                decoder=nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads), num_layers=6),\n",
    "                config=config,beam_size=10,max_length=256,sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/17/2024 18:25:07 - INFO - __main__ -   Test file: data/valid.java-cs.txt.java,data/valid.java-cs.txt.cs\n",
      "100%|██████████| 500/500 [00:00<00:00, 697.61it/s]\n",
      " 21%|██        | 13/63 [01:35<05:13,  6.28s/it]"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.to(device)\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to huggingface\n",
    "!huggingface-cli login\n",
    "repo_url = \"https://huggingface.co/judynguyen16/graphcodebert--code-translation-java-cs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Lightning Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10300 [00:00<?, ?it/s]11/20/2024 08:14:03 - INFO - __main__ -   *** Example ***\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Result', '_list', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_', '_(', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_ids: 0 15110 9527 29235 7529 35615 3999 35571 565 40981 48136 889 29235 7529 35615 3999 35571 565 40981 1437 36 9527 29235 7529 35615 3999 35571 565 40981 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 36583 29235 7529 35615 3999 35571 565 40981 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_code: (30, 31) (33, 34) (33, 34) (35, 39) (40, 41) (54, 55)\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', '(', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_ids: 0 15110 6229 9527 29235 7529 35615 3999 35571 565 40981 47806 9527 29235 7529 35615 3999 35571 565 40981 1640 36583 29235 7529 35615 3999 35571 565 40981 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 9527 29235 7529 35615 3999 35571 565 40981 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 36583 29235 7529 35615 3999 35571 565 40981 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   *** Example ***\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Update', 'J', 'ourney', 'State', 'Result', '_update', 'J', 'ourney', 'State', '_', '_(', '_Update', 'J', 'ourney', 'State', 'Request', '_request', '_)', '_{', '_request', '_=', '_before', 'Client', 'Exec', 'ution', '_(', '_request', '_)', '_;', '_return', '_execute', 'Update', 'J', 'ourney', 'State', '_(', '_request', '_)', '_;', '_}', '</s>', 'request', 'request', 'request', 'beforeClientExecution', 'request', 'request']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_ids: 0 15110 14686 863 37786 13360 48136 2935 863 37786 13360 1437 36 14686 863 37786 13360 45589 2069 4839 25522 2069 5457 137 47952 46891 15175 36 2069 4839 25606 671 11189 39962 863 37786 13360 36 2069 4839 25606 35524 2 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_code: (18, 19) (21, 22) (21, 22) (23, 27) (28, 29) (38, 39)\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_dfg: [] [3] [4] [] [0] [2]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_Update', 'J', 'ourney', 'State', 'Response', '_Update', 'J', 'ourney', 'State', '(', 'Update', 'J', 'ourney', 'State', 'Request', '_request', '){', 'var', '_options', '_=', '_new', '_Inv', 'oke', 'Options', '();', 'options', '.', 'Request', 'Marsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Request', 'Marsh', 'all', 'er', '.', 'Instance', ';', 'options', '.', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '_=', '_Update', 'J', 'ourney', 'State', 'Response', 'Un', 'm', 'arsh', 'all', 'er', '.', 'Instance', ';', 'return', '_Inv', 'oke', '<', 'Update', 'J', 'ourney', 'State', 'Response', '>(', 'request', ',', '_options', ');', '}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_ids: 0 15110 6229 14686 863 37786 13360 47806 14686 863 37786 13360 1640 39962 863 37786 13360 45589 2069 48512 10806 1735 5457 92 9318 5361 47261 47006 45012 4 45589 40825 1250 254 5457 14686 863 37786 13360 45589 40825 1250 254 4 49483 131 45012 4 47806 9685 119 14980 1250 254 5457 14686 863 37786 13360 47806 9685 119 14980 1250 254 4 49483 131 30921 9318 5361 41552 39962 863 37786 13360 47806 49925 16604 6 1735 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   *** Example ***\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_remove', 'Present', 'ation', 'Format', '_', '_(', '_)', '_{', '_remove', '1', 'st', 'Property', '_(', '_Property', 'ID', 'Map', '_.', '_PID', '_', 'PRES', 'FORM', 'AT', '_)', '_;', '_}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_ids: 0 15110 13842 3438 45195 1258 48587 1437 36 4839 25522 3438 134 620 44720 36 10491 2688 41151 479 47102 1215 29679 38036 2571 4839 25606 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_code: \n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_dfg: \n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_void', '_Remove', 'Present', 'ation', 'Format', '(){', 'M', 'utable', 'Section', '_s', '_=', '_(', 'M', 'utable', 'Section', ')', 'First', 'Section', ';', 's', '.', 'Remove', 'Property', '(', 'Property', 'ID', 'Map', '.', 'P', 'ID', '_', 'PRES', 'FORM', 'AT', ');', '}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_ids: 0 15110 13842 27336 45195 1258 48587 49215 448 41280 43480 579 5457 36 448 41280 43480 43 10993 43480 131 29 4 47583 44720 1640 44720 2688 41151 4 510 2688 1215 29679 38036 2571 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   *** Example ***\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '_(', '_int', '_first', 'Row', '_,', '_int', '_last', 'Row', '_,', '_int', '_first', 'Col', '_,', '_int', '_last', 'Col', '_)', '_{', '_this', '_(', '_)', '_;', '_add', 'Cell', 'Range', 'Address', '_(', '_first', 'Row', '_,', '_first', 'Col', '_,', '_last', 'Row', '_,', '_last', 'Col', '_)', '_;', '_}', '</s>', 'firstRow', 'lastRow', 'firstCol', 'lastCol', 'firstRow', 'firstCol', 'lastRow', 'lastCol']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_ids: 0 15110 13696 43430 46486 36583 36 6979 78 43277 2156 6979 94 43277 2156 6979 78 18551 2156 6979 94 18551 4839 25522 42 36 4839 25606 1606 40216 43430 46486 36 78 43277 2156 78 18551 2156 94 43277 2156 94 18551 4839 25606 35524 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_code: (8, 10) (12, 14) (16, 18) (20, 22) (33, 35) (36, 38) (39, 41) (42, 44)\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_dfg: [] [] [] [] [0] [2] [1] [3]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_Cell', 'Range', 'Address', 'List', '(', 'int', '_first', 'Row', ',', '_int', '_last', 'Row', ',', '_int', '_first', 'Col', ',', '_int', '_last', 'Col', '):', '_this', '(){', 'Add', 'Cell', 'Range', 'Address', '(', 'first', 'Row', ',', '_first', 'Col', ',', '_last', 'Row', ',', '_last', 'Col', ');', '}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_ids: 0 15110 13696 43430 46486 36583 1640 2544 78 43277 6 6979 94 43277 6 6979 78 18551 6 6979 94 18551 3256 42 49215 20763 40216 43430 46486 1640 9502 43277 6 78 18551 6 94 43277 6 94 18551 4397 24303 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   *** Example ***\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_tokens: ['<s>', 'public', '_void', '_delete', '_', '_(', '_int', '_key', '_)', '_{', '_int', '_i', '_=', '_binary', 'Search', '_(', '_m', 'Keys', '_,', '_0', '_,', '_m', 'Size', '_,', '_key', '_)', '_;', '_if', '_(', '_i', '_>=', '_0', '_)', '_{', '_if', '_(', '_m', 'Values', '_[', '_i', '_]', '_!=', '_DE', 'LET', 'ED', '_)', '_{', '_m', 'Values', '_[', '_i', '_]', '_=', '_DE', 'LET', 'ED', '_;', '_m', 'Gar', 'bage', '_=', '_true', '_;', '_}', '_}', '_}', '</s>', 'key', 'i', 'i', 'i', 'i', 'i', 'binarySearch', 'mKeys', '0', 'mSize', 'key', 'i', 'i', 'DELETED', 'mValues', 'i', 'DELETED']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_ids: 0 15110 13842 17462 1437 36 6979 762 4839 25522 6979 939 5457 32771 39954 36 475 44534 2156 321 2156 475 45698 2156 762 4839 25606 114 36 939 49095 321 4839 25522 114 36 475 48738 646 939 27779 49333 5885 24258 1691 4839 25522 475 48738 646 939 27779 5457 5885 24258 1691 25606 475 31626 36772 5457 1528 25606 35524 35524 35524 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   position_idx: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_code: (7, 8) (11, 12) (11, 12) (11, 12) (11, 12) (11, 12) (13, 15) (16, 18) (19, 20) (21, 23) (24, 25) (29, 30) (39, 40) (42, 45) (47, 49) (50, 51) (53, 56)\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   dfg_to_dfg: [] [6] [7] [8] [9] [10] [] [] [] [] [0] [5] [5] [] [16] [16] [13]\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_tokens: ['<s>', 'public', '_virtual', '_void', '_delete', '(', 'int', '_key', '){', 'int', '_i', '_=', '_binary', 'Search', '(', 'm', 'Keys', ',', '_0', ',', '_m', 'Size', ',', '_key', ');', 'if', '_(', 'i', '_>=', '_0', '){', 'if', '_(', 'm', 'Values', '[', 'i', ']', '_!=', '_DE', 'LET', 'ED', '){', 'm', 'Values', '[', 'i', ']', '_=', '_DE', 'LET', 'ED', ';', 'm', 'Gar', 'bage', '_=', '_true', ';', '}}}', '</s>']\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_ids: 0 15110 6229 13842 17462 1640 2544 762 48512 2544 939 5457 32771 39954 1640 119 44534 6 321 6 475 45698 6 762 4397 1594 36 118 49095 321 48512 1594 36 119 48738 10975 118 742 49333 5885 24258 1691 48512 119 48738 10975 118 742 5457 5885 24258 1691 131 119 31626 36772 5457 1528 131 49908 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/20/2024 08:14:03 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "100%|██████████| 10300/10300 [00:22<00:00, 466.73it/s]\n",
      "11/20/2024 08:14:25 - INFO - __main__ -   ***** Running training *****\n",
      "11/20/2024 08:14:25 - INFO - __main__ -     Num examples = 10300\n",
      "11/20/2024 08:14:25 - INFO - __main__ -     Batch size = 8\n",
      "11/20/2024 08:14:25 - INFO - __main__ -     Num epoch = 100\n"
     ]
    }
   ],
   "source": [
    "# ---------*****---------\n",
    "# Prepare training loader and fine-tuning\n",
    "train_examples = read_examples(args.train_filename)\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train')\n",
    "train_data = TextDataset(train_features,args)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps,num_workers=54)\n",
    "num_train_optimization_steps =  args.train_steps\n",
    "\n",
    "#Start training\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "logger.info(\"  Num epoch = %d\", args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 642.84it/s]\n",
      "11/20/2024 08:20:17 - INFO - __main__ -   \n",
      "***** Running evaluation *****\n",
      "11/20/2024 08:20:17 - INFO - __main__ -     Num examples = 500\n",
      "11/20/2024 08:20:17 - INFO - __main__ -     Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "# validation loader\n",
    "dev_dataset={}\n",
    "if 'dev_loss' in dev_dataset:\n",
    "    eval_examples,eval_data=dev_dataset['dev_loss']\n",
    "else:\n",
    "    eval_examples = read_examples(args.dev_filename)\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
    "    eval_data = TextDataset(eval_features,args)\n",
    "    dev_dataset['dev_loss']=eval_examples,eval_data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=54)\n",
    "\n",
    "logger.info(\"\\n***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/20/2024 08:22:53 - INFO - __main__ -   Test file: data/valid.java-cs.txt.java,data/valid.java-cs.txt.cs\n",
      "100%|██████████| 500/500 [00:00<00:00, 610.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "files=[]\n",
    "if args.dev_filename is not None:\n",
    "    files.append(args.dev_filename)\n",
    "if args.test_filename is not None:\n",
    "    files.append(args.test_filename)\n",
    "for idx,file in enumerate(files):   \n",
    "    logger.info(\"Test file: {}\".format(file))\n",
    "    eval_examples = read_examples(file)\n",
    "    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
    "    eval_data = TextDataset(eval_features,args) \n",
    "\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
